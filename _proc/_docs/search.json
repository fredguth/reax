[
  {
    "objectID": "stores.html",
    "href": "stores.html",
    "title": "stores",
    "section": "",
    "text": "source\n\nWritable\n\n Writable (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nReadable\n\n Readable (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nStore\n\n Store (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nStoreProtocol\n\n StoreProtocol (*args, **kwargs)\n\nThe Svelte Store contract protocol.\n\nsource\n\n\nsafe_not_equal\n\n safe_not_equal (a, b)\n\nCheck if a is not equal to b\n\nsource\n\n\nDerived\n\n Derived (s:Union[Store,list[Store]], fn:Callable)\n\nA Derived Store.\n\n\n\n\nType\nDetails\n\n\n\n\ns\nUnion[Store, listStore]\nsource store(s)\n\n\nfn\nCallable\na callback that takes the source store(s) values and returns the derived value\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nReadable\n\n Readable (initial_value:T, start:Notifier)\n\nA Readable Store.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nT\ninitial value of the store\n\n\nstart\nNotifier\nfunction called when the first subscriber is added\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nWritable\n\n Writable (initial_value:Any=None, start:Notifier=<function <lambda>>)\n\nA Writable Store.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninitial_value\nAny\nNone\ninitial value of the store\n\n\nstart\nNotifier\n\nA Notifier (Optional)\n\n\nReturns\nNone"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "From lovely-numpy: https://github.com/xl0/lovely-numpy/blob/master/nbs/03_utils.utils.ipynb\n\nsource\n\nansi_color\n\n ansi_color (s:str, col:str, use_color=True)\n\nVery minimal ANSI color support\n\nsource\n\n\nsparse_join\n\n sparse_join (lst, sep=' ')\n\n\nsource\n\n\npretty_str\n\n pretty_str (x)\n\nA slightly better way to print float-y values. Works for np.ndarray, torch.Tensor, jax.DeviceArray, and scalars.\n\nsource\n\n\nsci_mode\n\n sci_mode (f:float)\n\n\nsource\n\n\nto_str\n\n to_str (x, color=True, ddof=0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nInput\n\n\ncolor\nbool\nTrue\nANSI color highlighting\n\n\nddof\nint\n0\nFor “std” unbiasing\n\n\n\n\nto_str(jnp.array([[0.],[0.]]))\n\n'\\x1b[38;2;127;127;127mall_zeros\\x1b[0m'\n\n\n\nto_str(jnp.array([[1.],[4.],[3.]]))\n\n'x∈[1.000, 4.000] μ=2.667 σ=1.247'\n\n\n\nsource\n\n\nstr_tree\n\n str_tree (tree)"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "lj.monkey_patch()\njax.default_backend()\n\n'gpu'\n\n\n\nData\n\nsource\n\n\nDataLoaders\n\n DataLoaders (*dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\ncollate_dict\n\n collate_dict (ds)\n\n\nsource\n\n\nget_dls\n\n get_dls (train_ds, valid_ds, bs, **kwargs)\n\n\nXMEAN,XSTD, BATCH_SIZE, NUM_CLASSES = 0.28,0.35, 500, 10\n\ntfm = transforms.Compose([transforms.PILToTensor(), transforms.Lambda(lambda x: x/255), transforms.Normalize(XMEAN, XSTD), transforms.Lambda(lambda x: torch.flatten(x))])\nds = partial(torchvision.datasets.FashionMNIST,root=\"data\",download=True, transform = tfm)\ntrain_ds, valid_ds = ds(train=True), ds(train=False)\ntdl = DataLoader(train_ds, batch_size=BATCH_SIZE)\nvdl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\ndls = DataLoaders(tdl, vdl)\n\n\nsource\n\n\nBatch\n\n Batch (input:Union[jax.Array,numpy.ndarray],\n        target:Union[jax.Array,numpy.ndarray])\n\n\nbatch = Batch(*map(jnp.array, next(iter(dls.train))))\nbatch\n\nBatch(input=Array[500, 784] n=392000 x∈[-0.800, 2.057] μ=0.011 σ=1.006 gpu:0, target=Array[500] i32 x∈[0, 9] μ=4.402 σ=2.838 gpu:0)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "reax",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "reax",
    "section": "Install",
    "text": "Install\npip install reax"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "reax",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "As in miniai, we wil be using the FashionMnist Dataset for demonstration. Reax is not intended to be a complete library, the data module is just a copy from miniai to make it work.\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom reax.data import DataLoaders, Batch, Tensor\n\n\nXMEAN,XSTD, BATCH_SIZE, NUM_CLASSES = 0.28,0.35, 500, 10\n\ntfm = transforms.Compose([transforms.PILToTensor(), \n                          transforms.Lambda(lambda x: x/255), transforms.Normalize(XMEAN, XSTD), \n                          transforms.Lambda(lambda x: torch.flatten(x))])\nds = partial(torchvision.datasets.FashionMNIST,root=\"data\",download=True, transform = tfm)\ntrain_ds, valid_ds = ds(train=True), ds(train=False)\ntdl = DataLoader(train_ds, batch_size=BATCH_SIZE)\nvdl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\ndls = DataLoaders(tdl, vdl)\nbatch = Batch(*map(jnp.array, next(iter(dls.train))))\nbatch\n\n2023-03-01 20:50:50.805520: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n2023-03-01 20:50:50.814382: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr \n*** Begin stack trace ***\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    _PyObject_MakeTpCall\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    PyObject_Call\n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    PyObject_Call\n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    \n    PyObject_Call\n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    PyObject_Call\n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    PyObject_Call\n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    \n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    \n    \n    \n    _PyObject_MakeTpCall\n    _PyEval_EvalFrameDefault\n    \n    \n    \n    PyObject_Repr\n    PyUnicode_Format\n    _PyEval_EvalFrameDefault\n    \n    \n    \n    \n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    _PyObject_FastCallDictTstate\n    _PyObject_Call_Prepend\n    \n    _PyObject_MakeTpCall\n    _PyEval_EvalFrameDefault\n    \n    _PyFunction_Vectorcall\n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyObject_FastCallDictTstate\n    _PyObject_Call_Prepend\n    \n    _PyObject_MakeTpCall\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalCodeWithName\n    PyEval_EvalCodeEx\n    PyEval_EvalCode\n    \n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    _PyEval_EvalFrameDefault\n    \n    \n*** End stack trace ***\n\n\n\nXlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr \n\n\n\n\n\n\n\n\nNote\n\n\n\nHave you noticed the lj.monkey_patch() call? This is a call for lovely-jax, the wonderful library that makes the JAX array representation more friendly."
  },
  {
    "objectID": "core.html#model",
    "href": "core.html#model",
    "title": "core",
    "section": "Model",
    "text": "Model\nThe basic Haiku object to represent a model is a TransformedWithState. It represents a function or module that has been transformed by a hk.transform function. Here we are using hk.transform_with_state which is the superset of the transform functions.\nState in the Haiku lingo means everything that make your original Callable not a pure function. It is the context or state. Somoe common DNN modules like batch_normcan keep some state to perform its work. State, Buffers and Context are common names for this.\n\ndef forward(x:jnp.array) ->jnp.ndarray:\n  return hk.nets.MLP(output_sizes=[50,NUM_CLASSES])(x) # todo: remove NUM_CLASSES dependency\nnetwork = hk.transform_with_state(forward)\ntype(network)\n\nIn Reax, a Model is an immutable object. PyTrees are JAX datastructures.\n\nclass Model(NamedTuple):\n    params: PyTree # the models parameters, weights and biases\n    state: PyTree  # the model auxiliary state, e.g. batchnorm buffers\n    apply: ApplyFn # the model forward pass function\n    input_shape: Tuple[int, ...] # the shape of the input, used to infer the model output shape\n\n    rng = hk.PRNGSequence(42) # random number generator\n\n    @staticmethod\n    def from_haiku(\n        transformed: hk.TransformedWithState,       # transformed haiku model\n        x: Tensor                                   # example input (e.g. batch.input)\n    ):\n        ''' Create a Model from a Haiku Transformed object and an example input.'''\n        init, apply = transformed\n        params, state = jax.jit(init)(next(Model.rng), x)\n        return Model(params=params, state=state, apply=apply, input_shape=x.shape)\n\n\nsource\n\nModel\n\n Model (params:Union[jax.Array,numpy.ndarray,Tuple[ForwardRef('PyTree'),..\n        .],List[ForwardRef('PyTree')],Dict[Hashable,ForwardRef('PyTree')],\n        Mapping[str,Mapping[str,jax.Array]],Iterable[ForwardRef('ArrayTree\n        ')],Mapping[Any,ForwardRef('ArrayTree')],NoneType], state:Union[ja\n        x.Array,numpy.ndarray,Tuple[ForwardRef('PyTree'),...],List[Forward\n        Ref('PyTree')],Dict[Hashable,ForwardRef('PyTree')],Mapping[str,Map\n        ping[str,jax.Array]],Iterable[ForwardRef('ArrayTree')],Mapping[Any\n        ,ForwardRef('ArrayTree')],NoneType], apply:Callable[...,Tuple[Unio\n        n[jax.Array,numpy.ndarray],Union[jax.Array,numpy.ndarray,Tuple[For\n        wardRef('PyTree'),...],List[ForwardRef('PyTree')],Dict[Hashable,Fo\n        rwardRef('PyTree')],Mapping[str,Mapping[str,jax.Array]],Iterable[F\n        orwardRef('ArrayTree')],Mapping[Any,ForwardRef('ArrayTree')],NoneT\n        ype]]], input_shape:Tuple[int,...])\n\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nm\n\nLet’s keep us sane and improve the model representation.\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nm\n\n\nprint(m)\n\n\nModel Reactivity (Model Store)\nOk, now we will start to play with reactivity. In fastai (also in Keras, vanilla PyTorch, etc) there is the concept of Callbacks. It is the way to be notified when something of interest happens.\n\nDon’t nudge me, let me call you back when I have something for you!\n\nIn general, you will need a callback only during training, after all, it is when your things change. The model, the hyperparameters, the metrics, etc.\nThe fastai/miniai Learner is an Observable and you can hold multiple callbacks. Every callback keep its state in the Learner object. You can have callbacks for metrics, for logging and saving the training process… callbacks that depend on other callbacks! That is why there is that … shall I say… ugly order property in the Callbackclass.\nReax is just an experiment on how to handle this reactivity in another way. Maybe it will prove itself too bloated… or not. I decided to do it in JAX/Haiku to force a functional programming perspective.\nThe basic abstraction in Reax are stores, observables that hold any value. We could have used [RxPy] which is an incredible package. But its superpowers may be too much for what we need. That is why I took inspiration from the Svelte JS framework to create stores (it became its own package, Sveltish).\nA ModelStore is just a Writable store that holds values of type Model.\n\nsource\n\n\n\nModelStore\n\n ModelStore (initial_value:__main__.Model)\n\nA Model store. Custom Writable store\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nModel\nInitial value of the store\n\n\nReturns\nNone\n\n\n\n\n\nImproving the ModelStore representation\nWe also may improve its representation.\n\nms = ModelStore(m)\nms\n\n\nprint(ms)\n\nA callback is any Callable that you pass on subscribe.\n\nu1 = ms.subscribe(lambda x: print(\"1: callback 1\"))\n\nA change in the store value, triggers all callbacks subscribed to it.\n\nm = ms.get()\nms.set(Model(**(m._asdict()|{\"state\": {'a': 1, 'b': 2}})))\n\n\n\nOptimizer\nYou can have different stores for different things. For example, this is a simpler one to deal with the optimizer.\n\nsource\n\n\n\nOptimizer\n\n Optimizer (state:Union[jax.Array,Iterable[ForwardRef('ArrayTree')],Mappin\n            g[Any,ForwardRef('ArrayTree')]], apply:Callable)\n\nBy the way, we will use Optax, which is a good companion for Haiku.\n\ngrad_tfm = optax.sgd(1e-3)\napply = grad_tfm.update\noptState = grad_tfm.init(m.params) # you initialize the optimizer with the model params\noptimizer = Optimizer(state=optState, apply=apply)\noptimizer\n\n\nos= OptimizerStore(optimizer)\nu2 = os.subscribe(lambda x: print(f\"callback 2: {x}\"))\n\n\ngrad_tf2 = optax.adam(1e-4)\noptState2 = grad_tf2.init(m.params)\nos.set(Optimizer(state=optState2, apply=grad_tf2.update))\n\nCleaning up… you should remember to unsubscribe when you are done with a store.\n\nu1(), u2()\n\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nms = ModelStore(m)\nu1 = ms.subscribe(lambda x: print(f\"cb 1:\\n{x}\"))"
  },
  {
    "objectID": "core.html#training",
    "href": "core.html#training",
    "title": "core",
    "section": "Training",
    "text": "Training\nFinally we arrived at the Training, the core of the core ¯\\_(ツ)_/¯\nHere is where we will most need callbacks.\n\nLearner\nLike in fastai, we create a Learner class that will deal with the training.\n\nsource\n\n\nLearner\n\n Learner (model:__main__.ModelStore, dls:reax.data.DataLoaders, loss_func:\n          Callable[[Union[jax.Array,numpy.ndarray],Union[jax.Array,numpy.n\n          darray]],Union[jax.Array,numpy.ndarray]],\n          optimizer:reax.stores.Writable[__main__.Optimizer])\n\nBasic class for handling the training loop.\n\nlearner = Learner(model=ms, dls=dls, loss_func=optax.softmax_cross_entropy_with_integer_labels, optimizer=os)\nlearner\n\nLearner itself, is not a store, but holds different stores for different aspects of the training.\nWe have a ModelStore, an OptimizerStore… it is only missing the most important thing we want to observe… the training loop itself. We need a TrainingStore.\nBut for that… let’s first examine what we need. Let’s take a look in the training loop:\n\nFit: the training Loop\n# pseudo-code\n\ndef fit(epochs: int)->None:\n    '''Train the model for a number of epochs.'''\n    # before fit\n    for epoch in range(epochs):\n        # is_training\n        one_epoch(dls.train) # train for one epoch\n        # is_validating\n        one_epoch(dls.valid) # validate for one epoch\n        # should halt epochs?\n    # after fit\n\ndef one_epoch(dl)->None:\n    '''Train or validate for one epoch.'''\n    # before epoch\n    for batch_n, batch in enumerate(dl): \n        one_batch(batch_n, batch)\n        # should halt batches?\n    # after epoch\n\ndef one_batch(batch_n: int, batch: Batch)->None:\n    '''Train or validate for one batch.'''\n    # before batch\n    predict(...) # preds\n    evaluate(...)# loss\n    update model(...) if is_training\n    # after batch\nOur TrainingStore shall tell us where we are in the training loop and some information relevant at this point.\n\nI am training, epoch 5, iteration 345, after evaluate with certain current loss.\n\nAnother aspect is that it seems it should be a Readable store, afterall, we don’t want any callback being able to change information like: in which batch of which epoch am I?\nExceptionally, we want to tell the TrainingStore to halt.\nLet’s start with:\n\nsource\n\n\n\nTrainingState\n\n TrainingState (epochs:int, epoch:int, step:int, iter:int,\n                batch:Optional[reax.data.Batch], last:Dict=None,\n                is_running:bool=False, is_training:bool=False,\n                is_validating:bool=False, should_halt:bool=False)\n\n\nt = TrainingState(epochs=0, epoch=0, step=0, iter=0, batch=None)\nt\n\n\nsource\n\n\nTrainingStore\n\n TrainingStore (initial_value:T, start:Notifier)\n\nA store that keeps tracking of the training loop state\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nT\ninitial value of the store\n\n\nstart\nNotifier\nfunction called when the first subscriber is added\n\n\nReturns\nNone\n\n\n\n\n\nTrainingStore representation\n\n# a = [(\"A\", \"B\", \"C\"), (1,2,3)]\n# b = [(\"D\", \"E\", None), (4,5,None)]\na = [(\"A\", 1), [\"B\", 2], [\"C\", 3]]\nb = [[\"D\", 4], [\"E\", 5]]\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n\na = [[\"A\", 1], [\"B\", 2], [\"C\", 3]]\nb = []\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n\na = [('epoch', 0), ('step', 0), ('batch_n', 0), ('batch', None), ('metrics', None), ('last_event', None), ('is_training', False), ('should_halt', False)]\nb = [('0:', lambda:None)]\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n\nts = TrainingStore(t, lambda x:None)\nu4 = ts.subscribe(lambda x: print(f\"callback 4:\\n {x}\"))\n\n\nprint(ts)\n\n\nunsubs = []\nfor i in range(12):\n    u = ts.subscribe(lambda x: print(f\"callback: {i}\"))\n    unsubs.append(u)\nts\n\n\nfor u in unsubs: u()\n\n\n# print(state)\n\n\n# class TrainingStore(Writable[TrainingState]):\n\n\n# @fc.patch\n# def fit(self:Learner, n_epochs, trnState: TrainingState):\n#     \"Fit the model for `n_epochs` using batches from `dls`\"\n#     trnState.emit(Event(id=\"before_fit\", payload=None))\n#     for epoch in range(n_epochs):\n#         self.one_epoch(is_training=True, trnState=trnState)\n#         self.one_epoch(is_training=False, trnState=trnState)\n#         if (trnState.get().should_halt): break\n\n\n# training = TrainingStore(TrainingState(epoch=0, step=0, batch_n=0, batch=None, metrics=None, last_event=None))\n# u3 = training.subscribe(lambda x: print(f\"3:\\n {x}\"))\n\n\n# @fc.patch\n# def fit(self:Learner, n_epochs, trnState: TrainingState):\n#     \"Fit the model for `n_epochs` using batches from `dls`\"\n#     trnState.emit(Event(id=\"before_fit\", payload=None))\n#     for epoch in range(n_epochs):\n#         self.one_epoch(is_training=True, trnState=trnState)\n#         self.one_epoch(is_training=False, trnState=trnState)\n#         if (trnState.get().should_halt): break\n\n# @fc.patch\n# def one_epoch(self:Learner, is_training: bool, trnState: TrainingState):\n#     a = 1\n#     # print(f\"one_epoch: is_training={is_training}\")\n#     # print(trnState)\n#     # trnState._s_is_training = is_training\n#     # self.dl = self.dls.train if is_training else self.dls.valid\n#     # trnState.emit(Event(id=f\"before_epoch\", payload=trnState._s_epoch))\n#     # for batch_n, batch in enumerate(self.dl):\n#     #     trnState._s_batch_n, trnState._s_batch  = batch_n, batch\n#     #     # self.one_batch(trnState=trnState)\n#     #     if (trnState._s_should_halt): break\n#     # trnState.emit(Event(id=f\"after_epoch\", payload=trnState._s_epoch))\n\n\n# params, state, apply, _ = ms.get()\n# rng = hk.PRNGSequence(42) # random number generator\n# @jax.jit\n# def _predict(params, state, key, batch) -> Tensor:\n#     logits, new_state = apply(params, state, key, batch.input)\n#     return jnp.argmax (logits, axis=-1), new_state\n# key = next(rng)\n# _predict(params, state, key, batch)\n# @jax.jit\n# def _evaluate(params, state, key, batch) -> Tensor:\n#     preds, _ = _predict(params, state, key, batch)\n#     return jnp.mean(preds == batch.target)\n# from torch.utils.benchmark import Timer\n# evTimer = Timer(stmt=\"_evaluate(params, state, key, batch)\", globals=globals())\n# evTimer.timeit(1000)\n# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n\n# def evaluate(model: ModelStore, batch: Batch) -> Tensor:\n#     params, state, apply, _ = model.get()\n#     key = next(rng)\n#     return _evaluate(params, state, key, batch)\n# evaluate(ms, batch)\n# @jax.jit\n# def _loss_fn(params, state, key, batch)-> jnp.ndarray:\n#     targs = batch.target\n#     preds, new_state = apply(params, state, key, batch.input)\n#     # return the expectation of the loss wrt the distribution of the targets\n#     return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(preds, targs)/targs.shape[0]), new_state\n# key = next(rng)\n# loss, new_state = _loss_fn(params, state, key, batch)\n# lfTimer = Timer(stmt=\"_loss_fn(params, state, key, batch)\", globals=globals())\n# lfTimer.timeit(1000)\n\n\n# a = NamedTuple('A', [('a', int), ('b', int)])(1,2)\n# b = NamedTuple('A', [('a', int), ('b', int)])(3,3)\n# s1 = set(a._asdict().items())\n# s2 = set(b._asdict().items())\n# s1 ^ s2\n\n\n# trnState = TrainingStore(TrainingState(epoch=0, step=0, batch_n=0, batch=None, metrics=None, last_event=None))\n# logs = []\n# def logger(x):\n#     logs.append(x)\n#     last = set((logs[-1])._asdict().items())\n#     curr = set((x)._asdict().items())\n#     print (last ^ curr)\n\n# u4 = trnState.subscribe(lambda x: logger(x))\n\n\n# def one_batch(self):\n#     self.preds = self.model(self.batch[0])\n#     self.loss = self.loss_func(self.preds, self.batch[1])\n#     if self.model.training:\n#         self.loss.backward()\n#         self.opt.step()\n#         self.opt.zero_grad()\n\n\n# class Learner():\n#     def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n\n#     def one_batch(self):\n#         self.preds = self.model(self.batch[0])\n#         self.loss = self.loss_func(self.preds, self.batch[1])\n#         if self.model.training:\n#             self.loss.backward()\n#             self.opt.step()\n#             self.opt.zero_grad()\n\n#     def one_epoch(self, train):\n#         self.model.train(train)\n#         self.dl = self.dls.train if train else self.dls.valid\n#         try:\n#             self.callback('before_epoch')\n#             for self.iter,self.batch in enumerate(self.dl):\n#                 try:\n#                     self.callback('before_batch')\n#                     self.one_batch()\n#                     self.callback('after_batch')\n#                 except CancelBatchException: pass\n#             self.callback('after_epoch')\n#         except CancelEpochException: pass\n    \n#     def fit(self, n_epochs):\n#         self.n_epochs = n_epochs\n#         self.epochs = range(n_epochs)\n#         self.opt = self.opt_func(self.model.parameters(), self.lr)\n#         try:\n#             self.callback('before_fit')\n#             for self.epoch in self.epochs:\n#                 self.one_epoch(True)\n#                 self.one_epoch(False)\n#             self.callback('after_fit')\n#         except CancelFitException: pass\n\n#     def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n\n\n# #|export\n# class with_cbs:\n#     def __init__(self, nm): self.nm = nm\n#     def __call__(self, f):\n#         def _f(o, *args, **kwargs):\n#             try:\n#                 o.callback(f'before_{self.nm}')\n#                 f(o, *args, **kwargs)\n#                 o.callback(f'after_{self.nm}')\n#             except globals()[f'Cancel{self.nm.title()}Exception']: pass\n#             finally: o.callback(f'cleanup_{self.nm}')\n#         return _f\n\n\n# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n# params, state, apply, _ = ms.get()\n# @jax.jit\n# def _loss_fn(params, state, batch)-> Tuple[jnp.ndarray, PyTree]:\n#     bs, *_ = batch.target.shape\n#     logits, state = apply(params, state, next(rng), batch.input)\n#     state = {'a':1, 'b':2}\n#     return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(logits, batch.target)/bs)\n\n# def loss_fn(model: ModelStore, batch: Batch) -> float:\n#     params, state, apply, _ = model.get()\n#     loss_value =  _loss_fn(params, state, batch)\n#     new_model = Model(**(m._asdict()|{'state': new_state}))\n#     model.set(new_model)\n#     return float(loss_value)\n\n# loss_fn(ms, batch)\n# ms\n\n\n# from functools import partial\n\n\n# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n\n# def update(model: ModelStore, optimizer: OptimizerStore, batch: Batch)->None:\n#     m = model.get()\n#     o = optimizer.get()\n#     f = partial(loss_fn)(model=model)\n#     grads = jax.grad(loss_fn)(batch)\n#     @jax.jit\n#     def _update():\n#         updates, new_optState = o.apply(grads, o.state)\n#         new_model_params = optax.apply_updates(m.params, updates)\n#         return new_model_params, new_optState\n#     new_model_params, new_optState = _update()\n#     new_model = Model(**(m._asdict()|{'params': new_model_params}))\n#     new_optimizer = Optimizer(**(o._asdict()|{'state': new_optState}))\n#     model.set(new_model)\n#     optimizer.set(new_optimizer)\n#     return None\n\n\n# todo: tentar jax.tree_util.Partial\n\n\n# m = ms.get()\n# o = os.get()\n# f = partial(loss_fn, model=ms)\n# grads = jax.grad(f)(batch)\n# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n# params, state, apply, _ = ms.get()\n# def loss_fn():\n#     loss_value, new_state =  _loss_fn(params, state, batch)\n    \n# grads = jax.grad(_loss_fn)(params, state, batch)\n# grads\n# update(ms, os, batch)\n\n\n# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n\n# def loss_fn(model: ModelStore, batch: Batch) -> float:\n#     params, state, apply, _ = model.get()\n#     @jax.jit\n#     def _loss(params, state, batch)-> jnp.ndarray:\n#         bs, *_ = batch.target.shape\n#         logits, state = (apply)(params, state, next(rng), batch.input)\n#         return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(logits, batch.target)/bs), state\n#     loss_value, new_state =  _loss(params, state, batch)\n#     new_model = Model(**(m._asdict()|{'state': new_state}))\n#     model.set(new_model)\n#     return float(loss_value)\n\n# loss_fn(ms, batch)\n\n\n# def get_loss(loss_func, *args): return jax.jit(lambda params: loss_func(get_model(params), *args))\n# mse_loss = get_loss(mse, xb,tb) \n# mse_loss, mse_loss(W)\n# from torch.utils.benchmark import Timer\n# jax_grad = Timer( stmt=\"jax.grad(mse_loss)\", globals=globals())\n# jax_grad.timeit(1000)\n\n\n# class TrainingStore(Writable[TrainingState]):\n\n#     def emit(self, event: Event):\n#         self.set(self.value._replace(last_event=event))\n#     # def __getattr__(self, name): # there  is a bug, I can't fi\n#     #     if name[:3]=='_s_' : return getattr(self.value, name[3:])\n#     #     else: return super().__getattr__(name)\n#     # def __setattr__(self, name, value):\n#     #     if name[:3]=='_s_' and hasattr(self.value, name[3:]):\n#     #         self.set(self.value._replace(**{name[3:]: value}))\n#     #     else: super().__setattr__(name, value)\n#     def __repr__(self) -> str:\n#         return f\"{self.__class__.__name__}:\\n{self}\"\n#     def __str__(self) -> str:\n#         state = list(self.value._asdict().items())\n#         cbs = [(f\"{i}:\", v) for i, v in enumerate(self.subscribers)]\n#         table = list(itertools.zip_longest(list(zip(*state)),list(zip(*cbs))))\n#         return tabulate(table, headers=['State', 'Calbacks'], tablefmt='grid')\n\n\n# def __repr__(self) -> str:\n#         return f\"{self.__class__.__name__}:\\n{self}\"\n#     def __str__(self) -> str:\n#         state = list(self.value._asdict().items())\n#         state_t = list(zip(*state))\n#         cbs = [(f\"{i}:\", v) for i, v in enumerate(self.subscribers)]\n#         cbs_t = list(zip(*cbs))\n#         table = list(itertools.zip_longest(*state_t,*cbs_t))\n#         return tabulate(table, headers=['','State','', 'Calbacks'], tablefmt='grid')\n#     # @property\n#     # def _(self):\n#     #     \"\"\"The store value.\"\"\"\n#     #     return self.value\n#     # @_.setter\n#     # def _(self, value: TrainingState):\n#     #     self.set(value)"
  }
]