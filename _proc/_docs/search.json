[
  {
    "objectID": "stores.html",
    "href": "stores.html",
    "title": "stores",
    "section": "",
    "text": "source\n\nWritable\n\n Writable (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nReadable\n\n Readable (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nStore\n\n Store (**kwargs)\n\nA base class for all stores.\n\nsource\n\n\nStoreProtocol\n\n StoreProtocol (*args, **kwargs)\n\nThe Svelte Store contract protocol.\n\nsource\n\n\nsafe_not_equal\n\n safe_not_equal (a, b)\n\nCheck if a is not equal to b\n\nsource\n\n\nDerived\n\n Derived (s:Union[Store,list[Store]], fn:Callable)\n\nA Derived Store.\n\n\n\n\nType\nDetails\n\n\n\n\ns\nUnion[Store, listStore]\nsource store(s)\n\n\nfn\nCallable\na callback that takes the source store(s) values and returns the derived value\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nReadable\n\n Readable (initial_value:T, start:Notifier)\n\nA Readable Store.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nT\ninitial value of the store\n\n\nstart\nNotifier\nfunction called when the first subscriber is added\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nWritable\n\n Writable (initial_value:Any=None, start:Notifier=<function <lambda>>)\n\nA Writable Store.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninitial_value\nAny\nNone\ninitial value of the store\n\n\nstart\nNotifier\n\nA Notifier (Optional)\n\n\nReturns\nNone"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "From lovely-numpy: https://github.com/xl0/lovely-numpy/blob/master/nbs/03_utils.utils.ipynb\n\nsource\n\nansi_color\n\n ansi_color (s:str, col:str, use_color=True)\n\nVery minimal ANSI color support\n\nsource\n\n\nsparse_join\n\n sparse_join (lst, sep=' ')\n\n\nsource\n\n\npretty_str\n\n pretty_str (x)\n\nA slightly better way to print float-y values. Works for np.ndarray, torch.Tensor, jax.DeviceArray, and scalars.\n\nsource\n\n\nsci_mode\n\n sci_mode (f:float)\n\n\nsource\n\n\nto_str\n\n to_str (x, color=True, ddof=0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nInput\n\n\ncolor\nbool\nTrue\nANSI color highlighting\n\n\nddof\nint\n0\nFor “std” unbiasing\n\n\n\n\nto_str(jnp.array([[0.],[0.]]))\n\n'\\x1b[38;2;127;127;127mall_zeros\\x1b[0m'\n\n\n\nto_str(jnp.array([[1.],[4.],[3.]]))\n\n'x∈[1.000, 4.000] μ=2.667 σ=1.247'\n\n\n\nsource\n\n\nstr_tree\n\n str_tree (tree)"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "data",
    "section": "",
    "text": "lj.monkey_patch()\njax.default_backend()\n\n'gpu'\n\n\n\nData\n\nsource\n\n\nDataLoaders\n\n DataLoaders (*dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\ncollate_dict\n\n collate_dict (ds)\n\n\nsource\n\n\nget_dls\n\n get_dls (train_ds, valid_ds, bs, **kwargs)\n\n\nXMEAN,XSTD, BATCH_SIZE, NUM_CLASSES = 0.28,0.35, 500, 10\n\ntfm = transforms.Compose([transforms.PILToTensor(), transforms.Lambda(lambda x: x/255), transforms.Normalize(XMEAN, XSTD), transforms.Lambda(lambda x: torch.flatten(x))])\nds = partial(torchvision.datasets.FashionMNIST,root=\"data\",download=True, transform = tfm)\ntrain_ds, valid_ds = ds(train=True), ds(train=False)\ntdl = DataLoader(train_ds, batch_size=BATCH_SIZE)\nvdl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\ndls = DataLoaders(tdl, vdl)\n\n\nsource\n\n\nBatch\n\n Batch (input:Union[jax.Array,numpy.ndarray],\n        target:Union[jax.Array,numpy.ndarray])\n\n\nbatch = Batch(*map(jnp.array, next(iter(dls.train))))\nbatch\n\nBatch(input=Array[500, 784] n=392000 x∈[-0.800, 2.057] μ=0.011 σ=1.006 gpu:0, target=Array[500] i32 x∈[0, 9] μ=4.402 σ=2.838 gpu:0)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "reax",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "reax",
    "section": "Install",
    "text": "Install\npip install reax"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "reax",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "As in miniai, we wil be using the FashionMnist Dataset for demonstration. Reax is not intended to be a complete library, the data module is just a copy from miniai to make it work.\n\nimport torchvision\nimport torchvision.transforms as transforms\nfrom reax.data import DataLoaders, Batch, Tensor\n\n\nXMEAN,XSTD, BATCH_SIZE, NUM_CLASSES = 0.28,0.35, 500, 10\n\ntfm = transforms.Compose([transforms.PILToTensor(), \n                          transforms.Lambda(lambda x: x/255), transforms.Normalize(XMEAN, XSTD), \n                          transforms.Lambda(lambda x: torch.flatten(x))])\nds = partial(torchvision.datasets.FashionMNIST,root=\"data\",download=True, transform = tfm)\ntrain_ds, valid_ds = ds(train=True), ds(train=False)\ntdl = DataLoader(train_ds, batch_size=BATCH_SIZE)\nvdl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\ndls = DataLoaders(tdl, vdl)\nbatch = Batch(*map(jnp.array, next(iter(dls.train))))\nbatch\n\nBatch(input=Array[500, 784] n=392000 x∈[-0.800, 2.057] μ=0.011 σ=1.006 gpu:0, target=Array[500] i32 x∈[0, 9] μ=4.402 σ=2.838 gpu:0)\n\n\n\n\n\n\n\n\nNote\n\n\n\nHave you noticed how tensors are printed? This is lovely-jax, the wonderful library that makes the JAX array representation more friendly."
  },
  {
    "objectID": "core.html#model",
    "href": "core.html#model",
    "title": "core",
    "section": "Model",
    "text": "Model\nThe basic Haiku object to represent a model is a TransformedWithState. It represents a function or module that has been transformed by a hk.transform function. Here we are using hk.transform_with_state which is the superset of the transform functions.\nState in the Haiku lingo means everything that make your original Callable not a pure function. It is the context or state. Somoe common DNN modules like batch_normcan keep some state to perform its work. State, Buffers and Context are common names for this.\n\ndef forward(x:jnp.array) ->jnp.ndarray:\n  return hk.nets.MLP(output_sizes=[50,NUM_CLASSES])(x) # todo: remove NUM_CLASSES dependency\nnetwork = hk.transform_with_state(forward)\ntype(network)\n\nhaiku._src.transform.TransformedWithState\n\n\n\nModel class\nIn Reax, a Model is an immutable object. PyTrees are JAX datastructures.\n\nclass Model(NamedTuple):\n    params: PyTree # the models parameters, weights and biases\n    state: PyTree  # the model auxiliary state, e.g. batchnorm buffers\n    apply: ApplyFn # the model forward pass function\n    input_shape: Tuple[int, ...] # the shape of the input, used to infer the model output shape\n\n    rng = hk.PRNGSequence(42) # random number generator\n\n    @staticmethod\n    def from_haiku(\n        transformed: hk.TransformedWithState,       # transformed haiku model\n        x: Tensor                                   # example input (e.g. batch.input)\n    ):\n        ''' Create a Model from a Haiku Transformed object and an example input.'''\n        init, apply = transformed\n        params, state = jax.jit(init)(next(Model.rng), x)\n        return Model(params=params, state=state, apply=apply, input_shape=x.shape)\n\n\nsource\n\n\nModel\n\n Model (params:Union[jax.Array,numpy.ndarray,Tuple[ForwardRef('PyTree'),..\n        .],List[ForwardRef('PyTree')],Dict[Hashable,ForwardRef('PyTree')],\n        Mapping[str,Mapping[str,jax.Array]],Iterable[ForwardRef('ArrayTree\n        ')],Mapping[Any,ForwardRef('ArrayTree')],NoneType], state:Union[ja\n        x.Array,numpy.ndarray,Tuple[ForwardRef('PyTree'),...],List[Forward\n        Ref('PyTree')],Dict[Hashable,ForwardRef('PyTree')],Mapping[str,Map\n        ping[str,jax.Array]],Iterable[ForwardRef('ArrayTree')],Mapping[Any\n        ,ForwardRef('ArrayTree')],NoneType], apply:Callable[...,Tuple[Unio\n        n[jax.Array,numpy.ndarray],Union[jax.Array,numpy.ndarray,Tuple[For\n        wardRef('PyTree'),...],List[ForwardRef('PyTree')],Dict[Hashable,Fo\n        rwardRef('PyTree')],Mapping[str,Mapping[str,jax.Array]],Iterable[F\n        orwardRef('ArrayTree')],Mapping[Any,ForwardRef('ArrayTree')],NoneT\n        ype]]], input_shape:Tuple[int,...])\n\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nm\n\n\nModel(params={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] n=39200 x∈[-0.071, 0.071] μ=0.000 σ=0.032 gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] n=500 x∈[-0.276, 0.270] μ=-0.001 σ=0.121 gpu:0}}, state={}, apply=<function transform_with_state.<locals>.apply_fn at 0x7f514c0ed280>, input_shape=(500, 784))\n\n\n\nLet’s keep us sane and improve the model representation.\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nm\n\nModel:\n+---------------------------------------------+---------+\n| Params                                      | State   |\n+=============================================+=========+\n| mlp/~/linear_0:                             | {}      |\n|   b: all_zeros                              |         |\n|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |\n| mlp/~/linear_1:                             |         |\n|   b: all_zeros                              |         |\n|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |\n+---------------------------------------------+---------+\n\n\n\nprint(m)\n\n+--------------+-------------------------+-----------------+-------------+---------------+\n| Input        | Module                  | Module params   | Output      |   Param count |\n+==============+=========================+=================+=============+===============+\n| f32[500,784] | mlp (MLP)               |                 | f32[500,10] |        39,760 |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,784] | mlp/~/linear_0 (Linear) | w: f32[784,50]  | f32[500,50] |        39,250 |\n|              |  └ mlp (MLP)            | b: f32[50]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,50]  | mlp/~/linear_1 (Linear) | w: f32[50,10]   | f32[500,10] |           510 |\n|              |  └ mlp (MLP)            | b: f32[10]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n+---------------------------------------------+---------+\n| Params                                      | State   |\n+=============================================+=========+\n| mlp/~/linear_0:                             | {}      |\n|   b: all_zeros                              |         |\n|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |\n| mlp/~/linear_1:                             |         |\n|   b: all_zeros                              |         |\n|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |\n+---------------------------------------------+---------+\n\n\n\nModel Reactivity (Model Store)\nOk, now we will start to play with reactivity. In fastai (also in Keras, vanilla PyTorch, etc) there is the concept of Callbacks. It is the way to be notified when something of interest happens.\n\nDon’t nudge me, let me call you back when I have something for you!\n\nIn general, you will need a callback only during training, after all, it is when your things change. The model, the hyperparameters, the metrics, etc.\nThe fastai/miniai Learner is an Observable and you can hold multiple callbacks. Every callback keep its state in the Learner object. You can have callbacks for metrics, for logging and saving the training process… callbacks that depend on other callbacks! That is why there is that … shall I say… ugly order property in the Callbackclass.\nReax is just an experiment on how to handle this reactivity in another way. Maybe it will prove itself too bloated… or not. I decided to do it in JAX/Haiku to force a functional programming perspective.\nThe basic abstraction in Reax are stores, observables that hold any value. We could have used [RxPy] which is an incredible package. But its superpowers may be too much for what we need. That is why I took inspiration from the Svelte JS framework to create stores (it became its own package, Sveltish).\nA ModelStore is just a Writable store that holds values of type Model.\n\nsource\n\n\n\nModelStore\n\n ModelStore (initial_value:__main__.Model)\n\nA Model store. Custom Writable store\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nModel\nInitial value of the store\n\n\nReturns\nNone\n\n\n\n\n\nImproving the ModelStore representation\nWe also may improve its representation.\n\nms = ModelStore(m)\nms\n\nModelStore:\n+---------------------------------------------+---------+-------------+\n| Params                                      | State   | Callbacks   |\n+=============================================+=========+=============+\n| mlp/~/linear_0:                             | {}      | []          |\n|   b: all_zeros                              |         |             |\n|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |             |\n| mlp/~/linear_1:                             |         |             |\n|   b: all_zeros                              |         |             |\n|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |             |\n+---------------------------------------------+---------+-------------+\n\n\n\nprint(ms)\n\n+--------------+-------------------------+-----------------+-------------+---------------+\n| Input        | Module                  | Module params   | Output      |   Param count |\n+==============+=========================+=================+=============+===============+\n| f32[500,784] | mlp (MLP)               |                 | f32[500,10] |        39,760 |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,784] | mlp/~/linear_0 (Linear) | w: f32[784,50]  | f32[500,50] |        39,250 |\n|              |  └ mlp (MLP)            | b: f32[50]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,50]  | mlp/~/linear_1 (Linear) | w: f32[50,10]   | f32[500,10] |           510 |\n|              |  └ mlp (MLP)            | b: f32[10]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n+---------------------------------------------+---------+-------------+\n| Params                                      | State   | Callbacks   |\n+=============================================+=========+=============+\n| mlp/~/linear_0:                             | {}      | []          |\n|   b: all_zeros                              |         |             |\n|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |             |\n| mlp/~/linear_1:                             |         |             |\n|   b: all_zeros                              |         |             |\n|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |             |\n+---------------------------------------------+---------+-------------+\n\n\nA callback is any Callable that you pass on subscribe.\n\nu1 = ms.subscribe(lambda x: print(\"1: callback 1\"))\n\n1: callback 1\n\n\nA change in the store value, triggers all callbacks subscribed to it.\n\nm = ms.get()\nms.set(Model(**(m._asdict()|{\"state\": {'a': 1, 'b': 2}})))\n\n1: callback 1\n\n\n\n\nOptimizer\nYou can have different stores for different things. For example, this is a simpler one to deal with the optimizer.\n\nsource\n\n\n\nOptimizer\n\n Optimizer (state:Union[jax.Array,Iterable[ForwardRef('ArrayTree')],Mappin\n            g[Any,ForwardRef('ArrayTree')]], apply:Callable)\n\nBy the way, we will use Optax, which is a good companion for Haiku.\n\ngrad_tfm = optax.sgd(1e-3)\napply = grad_tfm.update\noptState = grad_tfm.init(m.params) # you initialize the optimizer with the model params\noptimizer = Optimizer(state=optState, apply=apply)\noptimizer\n\nOptimizer(state=(EmptyState(), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f512c6ab280>)\n\n\n\nos= OptimizerStore(optimizer)\nu2 = os.subscribe(lambda x: print(f\"callback 2: {x}\"))\n\ncallback 2: Optimizer(state=(EmptyState(), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f512c6ab280>)\n\n\n\ngrad_tf2 = optax.adam(1e-4)\noptState2 = grad_tf2.init(m.params)\nos.set(Optimizer(state=optState2, apply=grad_tf2.update))\n\ncallback 2: Optimizer(state=(ScaleByAdamState(count=Array i32 gpu:0 0, mu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}, nu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f512c6abca0>)\n\n\nCleaning up… you should remember to unsubscribe when you are done with a store.\n\nu1(), u2()\n\n(None, None)\n\n\n\nm = Model.from_haiku(transformed=network, x=batch.input)\nms = ModelStore(m)\nu1 = ms.subscribe(lambda x: print(f\"cb 1:\\n{x}\"))\n\ncb 1:\n+--------------+-------------------------+-----------------+-------------+---------------+\n| Input        | Module                  | Module params   | Output      |   Param count |\n+==============+=========================+=================+=============+===============+\n| f32[500,784] | mlp (MLP)               |                 | f32[500,10] |        39,760 |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,784] | mlp/~/linear_0 (Linear) | w: f32[784,50]  | f32[500,50] |        39,250 |\n|              |  └ mlp (MLP)            | b: f32[50]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n| f32[500,50]  | mlp/~/linear_1 (Linear) | w: f32[50,10]   | f32[500,10] |           510 |\n|              |  └ mlp (MLP)            | b: f32[10]      |             |               |\n+--------------+-------------------------+-----------------+-------------+---------------+\n+-----------------------------------------+---------+\n| Params                                  | State   |\n+=========================================+=========+\n| mlp/~/linear_0:                         | {}      |\n|   b: all_zeros                          |         |\n|   w: x∈[-0.071, 0.071] μ=-0.000 σ=0.032 |         |\n| mlp/~/linear_1:                         |         |\n|   b: all_zeros                          |         |\n|   w: x∈[-0.280, 0.278] μ=-0.008 σ=0.123 |         |\n+-----------------------------------------+---------+"
  },
  {
    "objectID": "core.html#training",
    "href": "core.html#training",
    "title": "core",
    "section": "Training",
    "text": "Training\nFinally we arrived at the Training, the core of the core ¯\\_(ツ)_/¯\nHere is where we will most need callbacks.\n\nLearner\nLike in fastai, we create a Learner class that will deal with the training.\n\nsource\n\n\nLearner\n\n Learner (model:__main__.ModelStore, dls:reax.data.DataLoaders, loss_func:\n          Callable[[Union[jax.Array,numpy.ndarray],Union[jax.Array,numpy.n\n          darray]],Union[jax.Array,numpy.ndarray]],\n          optimizer:reax.stores.Writable[__main__.Optimizer])\n\nBasic class for handling the training loop.\n\nlearner = Learner(model=ms, dls=dls, loss_func=optax.softmax_cross_entropy_with_integer_labels, optimizer=os)\nlearner\n\nLearner:\n+-----------------+-----------------+-----------------+-----------------+\n|           Model |     DataLoaders |          LossFn |       Optimizer |\n+=================+=================+=================+=================+\n| 139986614063552 | 139987146199584 | 139990720158448 | 139986614065904 |\n+-----------------+-----------------+-----------------+-----------------+\n\n\nLearner itself, is not a store, but holds different stores for different aspects of the training.\nWe have a ModelStore, an OptimizerStore… it is only missing the most important thing we want to observe… the training loop itself. We need a TrainingStore.\nBut for that… let’s first examine what we need. Let’s take a look in the training loop:\n\nThe anatomy of a training loop\n# pseudo-code\n\ndef fit(epochs: int)->None:\n    '''Train the model for a number of epochs.'''\n    # before fit\n    for epoch in range(epochs):\n        # is_training\n        one_epoch(dls.train) # train for one epoch\n        # is_validating\n        one_epoch(dls.valid) # validate for one epoch\n        # should halt epochs?\n    # after fit\n\ndef one_epoch(dl)->None:\n    '''Train or validate for one epoch.'''\n    # before epoch\n    for batch_n, batch in enumerate(dl): \n        one_batch(batch_n, batch)\n        # should halt batches?\n    # after epoch\n\ndef one_batch(batch_n: int, batch: Batch)->None:\n    '''Train or validate for one batch.'''\n    # before batch\n    predict(...) # preds\n    evaluate(...)# loss\n    update model(...) if is_training\n    # after batch\nOur TrainingStore shall tell us where we are in the training loop and some information relevant at this point.\n\nI am training, epoch 5, iteration 345, after evaluate with certain current loss.\n\nAnother aspect is that it seems it should be a Readable store, afterall, we don’t want any callback being able to change information like: in which batch of which epoch am I?\nExceptionally, we want to tell the TrainingStore to halt.\nLet’s start with:\n\nsource\n\n\n\nTrainingState\n\n TrainingState (epochs:int, epoch:int, step:int, iter:int,\n                batch:Optional[reax.data.Batch], last:Dict=None,\n                is_running:bool=False, is_training:bool=False,\n                is_validating:bool=False, should_halt:bool=False)\n\n\nt = TrainingState(epochs=0, epoch=0, step=0, iter=0, batch=None)\nt\n\nTrainingState:\n-------------  -----\nepochs             0\nepoch              0\nstep               0\niter               0\nbatch\nlast\nis_running     False\nis_training    False\nis_validating  False\nshould_halt    False\n-------------  -----\n\n\n\n\nTraining Store\n\nsource\n\n\nTrainingStore\n\n TrainingStore (initial_value:T, start:Notifier)\n\nA store that keeps tracking of the training loop state\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninitial_value\nT\ninitial value of the store\n\n\nstart\nNotifier\nfunction called when the first subscriber is added\n\n\nReturns\nNone\n\n\n\n\n\nTrainingStore representation\n\n# a = [(\"A\", \"B\", \"C\"), (1,2,3)]\n# b = [(\"D\", \"E\", None), (4,5,None)]\na = [(\"A\", 1), [\"B\", 2], [\"C\", 3]]\nb = [[\"D\", 4], [\"E\", 5]]\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n+----+------+----+------+\n|    |   H1 |    |   H2 |\n+====+======+====+======+\n| A  |    1 | D  |    4 |\n+----+------+----+------+\n| B  |    2 | E  |    5 |\n+----+------+----+------+\n| C  |    3 |    |      |\n+----+------+----+------+\n\n\n\na = [[\"A\", 1], [\"B\", 2], [\"C\", 3]]\nb = []\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n+----+------+\n|    |   H1 |\n+====+======+\n| A  |    1 |\n+----+------+\n| B  |    2 |\n+----+------+\n| C  |    3 |\n+----+------+\n\n\n\na = [('epoch', 0), ('step', 0), ('batch_n', 0), ('batch', None), ('metrics', None), ('last_event', None), ('is_training', False), ('should_halt', False)]\nb = [('0:', lambda:None)]\nc = list(zip(*a))\nd = list(zip(*b))\ntable = list(itertools.zip_longest(*c,*d))\nprint(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))\n\n+-------------+-------+----+---------------------------------------+\n|             |    H1 |    | H2                                    |\n+=============+=======+====+=======================================+\n| epoch       |     0 | 0: | <function <lambda> at 0x7f514c0ed820> |\n+-------------+-------+----+---------------------------------------+\n| step        |     0 |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| batch_n     |     0 |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| batch       |       |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| metrics     |       |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| last_event  |       |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| is_training | False |    |                                       |\n+-------------+-------+----+---------------------------------------+\n| should_halt | False |    |                                       |\n+-------------+-------+----+---------------------------------------+\n\n\n\nts = TrainingStore(t, lambda x:None)\nu4 = ts.subscribe(lambda x: print(f\"callback 4:\\n {x}\"))\n\ncallback 4:\n -------------  -----\nepochs             0\nepoch              0\nstep               0\niter               0\nbatch\nlast\nis_running     False\nis_training    False\nis_validating  False\nshould_halt    False\n-------------  -----\n\n\n\nprint(ts)\n\n+---------------+---------+----+---------------------------------------+\n|               |   State |    | Calbacks                              |\n+===============+=========+====+=======================================+\n| epochs        |       0 | 0: | <function <lambda> at 0x7f512c3d5af0> |\n+---------------+---------+----+---------------------------------------+\n| epoch         |       0 |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| step          |       0 |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| iter          |       0 |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| batch         |         |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| last          |         |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| is_running    |   False |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| is_training   |   False |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| is_validating |   False |    |                                       |\n+---------------+---------+----+---------------------------------------+\n| should_halt   |   False |    |                                       |\n+---------------+---------+----+---------------------------------------+\n\n\n\nunsubs = []\nfor i in range(12):\n    u = ts.subscribe(lambda x: print(f\"callback: {i}\"))\n    unsubs.append(u)\nts\n\ncallback: 0\ncallback: 1\ncallback: 2\ncallback: 3\ncallback: 4\ncallback: 5\ncallback: 6\ncallback: 7\ncallback: 8\ncallback: 9\ncallback: 10\ncallback: 11\n\n\nTrainingStore:\n+---------------+---------+-----+---------------------------------------+\n|               |   State |     | Calbacks                              |\n+===============+=========+=====+=======================================+\n| epochs        |       0 | 0:  | <function <lambda> at 0x7f512c3f6430> |\n+---------------+---------+-----+---------------------------------------+\n| epoch         |       0 | 1:  | <function <lambda> at 0x7f512c3d5e50> |\n+---------------+---------+-----+---------------------------------------+\n| step          |       0 | 2:  | <function <lambda> at 0x7f512c3d5670> |\n+---------------+---------+-----+---------------------------------------+\n| iter          |       0 | 3:  | <function <lambda> at 0x7f512c3f6670> |\n+---------------+---------+-----+---------------------------------------+\n| batch         |         | 4:  | <function <lambda> at 0x7f512c3f60d0> |\n+---------------+---------+-----+---------------------------------------+\n| last          |         | 5:  | <function <lambda> at 0x7f512c3d5af0> |\n+---------------+---------+-----+---------------------------------------+\n| is_running    |   False | 6:  | <function <lambda> at 0x7f512c3f6310> |\n+---------------+---------+-----+---------------------------------------+\n| is_training   |   False | 7:  | <function <lambda> at 0x7f512c3d5d30> |\n+---------------+---------+-----+---------------------------------------+\n| is_validating |   False | 8:  | <function <lambda> at 0x7f512c3f6550> |\n+---------------+---------+-----+---------------------------------------+\n| should_halt   |   False | 9:  | <function <lambda> at 0x7f512c3d5160> |\n+---------------+---------+-----+---------------------------------------+\n|               |         | 10: | <function <lambda> at 0x7f512c3d5f70> |\n+---------------+---------+-----+---------------------------------------+\n|               |         | 11: | <function <lambda> at 0x7f512c3d5790> |\n+---------------+---------+-----+---------------------------------------+\n|               |         | 12: | <function <lambda> at 0x7f512c3f61f0> |\n+---------------+---------+-----+---------------------------------------+\n\n\n\nfor u in unsubs: u()\n\n\na = NamedTuple(\"a\", [(\"n\", int)])\na.n = 1\n\n\na.x = 3\nprint(a)\n\n<class '__main__.a'>\n\n\n\nclass Bunch:\n    __init__ = lambda self, **kw: setattr(self, '__dict__', kw)\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.__dict__!r})\"\n\n\na = Bunch(n=2,x=4)\n\n\na.y=10\na.iter = 20\n\n\na\n\nBunch({'n': 2, 'x': 4, 'y': 10, 'iter': 20})\n\n\n\na.__dict__|{'y':12}\n\n{'n': 2, 'x': 4, 'y': 12, 'iter': 20, 'z': 30}\n\n\n\nsetattr(a, 'z', 30)\na\n\nBunch({'n': 2, 'x': 4, 'y': 10, 'iter': 20, 'z': 30})\n\n\n\nclass with_interceptor:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        print('intercepting....')\n        print(f)\n        # print(f'args: {args}')\n        # print(f'kwargs: {kwargs}')\n        return f\n    def __setattr__(self, k,v) -> None:\n        print('i.__setattr__:', (k,v))\n        super().__setattr__(k,v)\n\nclass DummyCls:\n\n    @with_interceptor\n    def dummy_fn(self, x):\n        x = 1\n        print('dummy')\n\na = DummyCls()\na.dummy_fn()\n\ni.__setattr__: ('nm', <function DummyCls.dummy_fn at 0x7f50e4388160>)\n\n\nTypeError: __call__() missing 1 required positional argument: 'f'\n\n\n\ndef make_pretty(func):\n    def inner(*args, **kwargs):\n        print(\"I got decorated\")\n        func(*args, **kwargs)\n    return inner\n\n\n@make_pretty\ndef ordinary(x):\n    print(\"I am ordinary\")\n\n\nordinary(1)\n\nI got decorated\nI am ordinary\n\n\n\nclass make_pretty:\n    def __init__(self, f):self.f = f\n\n    def __call__(self, *args, **kwargs):\n        self.args = list(args)\n        self.kwargs = dict(kwargs)\n        print(\"I got decorated\")\n        self.f(*args, **kwargs)\n\n    def __setattr__(self, k,v) -> None:\n        print('intercepting:', (k,v))\n        super().__setattr__(k,v)\n\n\n#     def inner(*args, **kwargs):\n#         print(\"I got decorated\")\n#         func(*args, **kwargs)\n#     return inner\n\n\n@make_pretty\ndef ordinary(x):\n    x = 3\n    print(\"I am ordinary \", x)\n\nintercepting: ('f', <function ordinary at 0x7f50e43c0310>)\n\n\n\nordinary(1)\n\nintercepting: ('args', [1])\nintercepting: ('kwargs', {})\nI got decorated\nI am ordinary  3\n\n\n\nclass dummyInterceptor:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(*args, **kwargs):\n            f(*args, **kwargs)\n        return _f\n\n    def __setattr__(self, k,v) -> None:\n        print('intercepting:', (k,v))\n        super().__setattr__(k,v)\n\n# def __call__(self, f):\n#         def _f(o, *args, **kwargs):\n#             try:\n#                 o.callback(f'before_{self.nm}')\n#                 f(o, *args, **kwargs)\n#                 o.callback(f'after_{self.nm}')\n#             except globals()[f'Cancel{self.nm.title()}Exception']: pass\n#             finally: o.callback(f'cleanup_{self.nm}')\n#         return _f\n\n\n@dummyInterceptor\ndef dummy(x):\n    x = 1\n    def g():\n        y = 2\n\nintercepting: ('nm', <function dummy at 0x7f50e4412c10>)\n\n\n\ndummy()\n\nTypeError: __call__() missing 1 required positional argument: 'f'\n\n\n\nimport inspect\n\n\nf.__code__.co_varnames\n\n('x', 'y', 'z', 'a', 'g', 'c', 'd')\n\n\n\nf.__code__.co_varnames\n\n('x', 'y', 'z', 'a', 'g', 'c', 'd')\n\n\n\nlearner.__dict__\n\n\n\n{'__stored_args__': {'model': ModelStore:\n  +-----------------------------------------+---------+--------------------------------------------+\n  | Params                                  | State   | Callbacks                                  |\n  +=========================================+=========+============================================+\n  | mlp/~/linear_0:                         | {}      | - 0: <function <lambda> at 0x7f512c6abf70> |\n  |   b: all_zeros                          |         |                                            |\n  |   w: x∈[-0.071, 0.071] μ=-0.000 σ=0.032 |         |                                            |\n  | mlp/~/linear_1:                         |         |                                            |\n  |   b: all_zeros                          |         |                                            |\n  |   w: x∈[-0.280, 0.278] μ=-0.008 σ=0.123 |         |                                            |\n  +-----------------------------------------+---------+--------------------------------------------+,\n  'dls': <reax.data.DataLoaders at 0x7f514c1f5220>,\n  'loss_func': <function optax._src.loss.softmax_cross_entropy_with_integer_labels(logits: jax.Array, labels: jax.Array) -> jax.Array>,\n  'optimizer': Writable(Optimizer(state=(ScaleByAdamState(count=Array i32 gpu:0 0, mu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}, nu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f512c6abca0>))},\n 'model': ModelStore:\n +-----------------------------------------+---------+--------------------------------------------+\n | Params                                  | State   | Callbacks                                  |\n +=========================================+=========+============================================+\n | mlp/~/linear_0:                         | {}      | - 0: <function <lambda> at 0x7f512c6abf70> |\n |   b: all_zeros                          |         |                                            |\n |   w: x∈[-0.071, 0.071] μ=-0.000 σ=0.032 |         |                                            |\n | mlp/~/linear_1:                         |         |                                            |\n |   b: all_zeros                          |         |                                            |\n |   w: x∈[-0.280, 0.278] μ=-0.008 σ=0.123 |         |                                            |\n +-----------------------------------------+---------+--------------------------------------------+,\n 'dls': <reax.data.DataLoaders at 0x7f514c1f5220>,\n 'loss_func': <function optax._src.loss.softmax_cross_entropy_with_integer_labels(logits: jax.Array, labels: jax.Array) -> jax.Array>,\n 'optimizer': Writable(Optimizer(state=(ScaleByAdamState(count=Array i32 gpu:0 0, mu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}, nu={'mlp/~/linear_0': {'b': Array[50] all_zeros gpu:0, 'w': Array[784, 50] all_zeros gpu:0}, 'mlp/~/linear_1': {'b': Array[10] all_zeros gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] all_zeros gpu:0}}), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f512c6abca0>))}\n\n\n\n\n\n\n'__main__'\n\n\n\n@fc.patch\ndef __getattr__(self:Learner, name):\n        if name in ('epochs','epoch','iter','one_batch'): \n            return partial(self.callback, name)\n        raise AttributeError(name)\n\nAttributeError: 'Learner' object has no attribute 'getattrs'\n\n\n\nclass with_cbs:\n    def __init__(self, nm): self.nm = nm\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.callback(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.callback(f'after_{self.nm}')\n            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n            finally: o.callback(f'cleanup_{self.nm}')\n        return _f\n\n\ndef callback(self, method_nm): \n    run_cbs(self.cbs, method_nm, self)"
  }
]