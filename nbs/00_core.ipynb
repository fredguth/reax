{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This is the core of the `Reax` lib. \n",
    "\n",
    "Here we define the major abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.2\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from functools import partial\n",
    "# from itertools import zip_longest\n",
    "from typing import (Callable, Dict, Hashable, List, Mapping, NamedTuple,\n",
    "                    Optional, Sequence, Tuple, Union)\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "import lovely_jax as lj\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "lj.monkey_patch()\n",
    "jax.default_backend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in `miniai`, we wil be using the `FashionMnist` Dataset for demonstration.   `Reax` is not intended to be a complete library, the `data` module is just a copy from [miniai]() to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(input=Array[500, 784] n=392000 x∈[-0.800, 2.057] μ=0.011 σ=1.006 gpu:0, target=Array[500] i32 x∈[0, 9] μ=4.402 σ=2.838 gpu:0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from reax.data import DataLoaders, Batch, Tensor\n",
    "\n",
    "XMEAN,XSTD, BATCH_SIZE, NUM_CLASSES = 0.28,0.35, 500, 10\n",
    "\n",
    "tfm = transforms.Compose([transforms.PILToTensor(), transforms.Lambda(lambda x: x/255), transforms.Normalize(XMEAN, XSTD), transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "ds = partial(torchvision.datasets.FashionMNIST,root=\"data\",download=True, transform = tfm)\n",
    "train_ds, valid_ds = ds(train=True), ds(train=False)\n",
    "tdl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "vdl = DataLoader(valid_ds, batch_size=BATCH_SIZE)\n",
    "dls = DataLoaders(tdl, vdl)\n",
    "batch = Batch(*map(jnp.array, next(iter(dls.train))))\n",
    "batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic [Haiku](https://dm-haiku.readthedocs.io/) object to represent a model is a [TransformedWithState](https://dm-haiku.readthedocs.io/en/latest/api.html#transformedwithstate).  It represents a `function` or `module` that has been transformed by a `hk.transform` function.  Here we are using `hk.transform_with_state` which is the superset of the transform functions.  \n",
    "\n",
    "State in the `Haiku` lingo means everything that make your original `Callable` not a pure function.  It is the context or state.  Somoe common `DNN` modules like `batch_norm`can keep some `state` to perform its work.  `State`, `Buffers` and `Context` are common names for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "haiku._src.transform.TransformedWithState"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(x:jnp.array) ->jnp.ndarray:\n",
    "  return hk.nets.MLP(output_sizes=[50,NUM_CLASSES])(x) # todo: remove NUM_CLASSES dependency\n",
    "network = hk.transform_with_state(forward)\n",
    "type(network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Reax`, a `Model` is an immutable object. [PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html) are JAX datastructures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTree = Union[\n",
    "    Tensor, Tuple[\"PyTree\", ...], List[\"PyTree\"], Dict[Hashable, \"PyTree\"], hk.Params, hk.State, optax.OptState, None\n",
    "]  # I hope that with this definition it will work in  Haiku and Flax\n",
    "\n",
    "ApplyFn = Callable[..., Tuple[Tensor, PyTree]] # returns result and state (aka buffers)\n",
    "\n",
    "class Model(NamedTuple):\n",
    "    params: PyTree # the models parameters, weights and biases\n",
    "    state: PyTree  # the model auxiliary state, e.g. batchnorm buffers\n",
    "    apply: ApplyFn # the model forward pass function\n",
    "    input_shape: Tuple[int, ...] # the shape of the input, used to infer the model output shape\n",
    "\n",
    "    rng = hk.PRNGSequence(42) # random number generator\n",
    "\n",
    "    @staticmethod\n",
    "    def from_haiku(\n",
    "        transformed: hk.TransformedWithState,       # transformed haiku model\n",
    "        x: Tensor                                   # example input (e.g. batch.input)\n",
    "    ):\n",
    "        ''' Create a Model from a Haiku Transformed object and an example input.'''\n",
    "        init, apply = transformed\n",
    "        params, state = jax.jit(init)(next(Model.rng), x)\n",
    "        return Model(params=params, state=state, apply=apply, input_shape=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(params={'mlp/~/linear_0': {'b': Array[50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[784, 50] n=39200 x∈[-0.071, 0.071] μ=0.000 σ=0.032 gpu:0}, 'mlp/~/linear_1': {'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] n=500 x∈[-0.276, 0.270] μ=-0.001 σ=0.121 gpu:0}}, state={}, apply=<function transform_with_state.<locals>.apply_fn at 0x7f02d8117670>, input_shape=(500, 784))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model.from_haiku(transformed=network, x=batch.input)\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep us sane and improve the model representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import fastcore.all as fc\n",
    "from tabulate import tabulate\n",
    "from reax.utils import str_tree\n",
    "# the tabulate package is also used by Haiku in its hk.experimental methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def __repr__(self:Model)->str:\n",
    "    table = [[\"Params\", \"State\"],[str_tree(self.params), str_tree(self.state)]]\n",
    "    return f\"{self.__class__.__name__}:\\n{tabulate(table, headers='firstrow', tablefmt='grid')}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model:\n",
       "+---------------------------------------------+---------+\n",
       "| Params                                      | State   |\n",
       "+=============================================+=========+\n",
       "| mlp/~/linear_0:                             | {}      |\n",
       "|   b: all_zeros                              |         |\n",
       "|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |\n",
       "| mlp/~/linear_1:                             |         |\n",
       "|   b: all_zeros                              |         |\n",
       "|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |\n",
       "+---------------------------------------------+---------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model.from_haiku(transformed=network, x=batch.input)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def __str__(self:Model) -> str:\n",
    "    s1 = hk.experimental.tabulate(self.apply,\n",
    "            columns=[\"input\", \"module\", \"owned_params\", \"output\", \"params_size\"])(jnp.ones(self.input_shape))\n",
    "    s2 = '\\n'.join(self.__repr__().split('\\n')[1:])\n",
    "    return f\"{s1}\\n{s2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| Input        | Module                  | Module params   | Output      |   Param count |\n",
      "+==============+=========================+=================+=============+===============+\n",
      "| f32[500,784] | mlp (MLP)               |                 | f32[500,10] |        39,760 |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| f32[500,784] | mlp/~/linear_0 (Linear) | w: f32[784,50]  | f32[500,50] |        39,250 |\n",
      "|              |  └ mlp (MLP)            | b: f32[50]      |             |               |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| f32[500,50]  | mlp/~/linear_1 (Linear) | w: f32[50,10]   | f32[500,10] |           510 |\n",
      "|              |  └ mlp (MLP)            | b: f32[10]      |             |               |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "+---------------------------------------------+---------+\n",
      "| Params                                      | State   |\n",
      "+=============================================+=========+\n",
      "| mlp/~/linear_0:                             | {}      |\n",
      "|   b: all_zeros                              |         |\n",
      "|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |\n",
      "| mlp/~/linear_1:                             |         |\n",
      "|   b: all_zeros                              |         |\n",
      "|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |\n",
      "+---------------------------------------------+---------+\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Reactivity (Model Store)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we will start to play with reactivity.  In `fastai` (also in Keras, vanilla PyTorch, etc) there is the concept of `Callbacks`.  It is the way to be notified when something of interest happens. \n",
    "\n",
    "> Don't nudge me, let me __call you back__ when I have something for you!\n",
    "\n",
    "In general, you will need a callback only during training, after all, it is when your `things` change.  The model, the hyperparameters, the metrics, etc.\n",
    "\n",
    "The __fastai/miniai__ `Learner` is an `Observable` and you can hold multiple callbacks. Every callback keep its state in the Learner object. You can have callbacks for metrics, for logging and saving the training process... callbacks that depend on other callbacks! That is why there is that ... shall I say... __ugly__ `order` property in the `Callback`class.\n",
    "\n",
    "`Reax` is just an experiment on how to handle this reactivity in another way.  Maybe it will prove itself too bloated... or not. I decided to do it in `JAX/Haiku` to force a `functional programming` perspective.\n",
    "\n",
    "The basic abstraction in  `Reax` are `stores`, observables that hold any value. We could have used [RxPy] which is an incredible package. But its superpowers may be too much for what we need. That is why I took inspiration from the `Svelte` JS framework to create `stores` (it became its own package, [Sveltish](https://fredguth.github.io/sveltish)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from reax.stores import Writable, Notifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ModelStore` is just a `Writable` store that holds values of type `Model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStore(Writable[Model]):\n",
    "    ''' A Model store. Custom Writable store'''\n",
    "    def __init__(self,\n",
    "                initial_value: Model, # Initial value of the store\n",
    "            ) -> None:\n",
    "        start: Notifier = lambda x: None # we won't need a Start/Stop Notifier\n",
    "        super().__init__(initial_value, start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving the ModelStore representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also may improve its representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def __repr__(self:ModelStore) -> str:\n",
    "    params, state, apply, shape = self.value\n",
    "    table = [[\"Params\", \"State\", \"Callbacks\"],[str_tree(params), str_tree(state), [f\"{s}\\n\" for s in self.subscribers]]]\n",
    "    table = [[\"Params\", \"State\", \"Callbacks\"],\n",
    "                [str_tree(m.params), str_tree(m.state), yaml.dump([{i:str(f)} for i,f in enumerate(self.subscribers)])]]\n",
    "    return f\"{self.__class__.__name__}:\\n{tabulate(table, headers='firstrow', tablefmt='grid')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelStore:\n",
       "+---------------------------------------------+---------+-------------+\n",
       "| Params                                      | State   | Callbacks   |\n",
       "+=============================================+=========+=============+\n",
       "| mlp/~/linear_0:                             | {}      | []          |\n",
       "|   b: all_zeros                              |         |             |\n",
       "|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |             |\n",
       "| mlp/~/linear_1:                             |         |             |\n",
       "|   b: all_zeros                              |         |             |\n",
       "|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |             |\n",
       "+---------------------------------------------+---------+-------------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = ModelStore(m)\n",
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def __str__(self:ModelStore) -> str:\n",
    "    columns=[\"input\", \"module\", \"owned_params\", \"output\", \"params_size\"]\n",
    "    s = hk.experimental.tabulate(self.value.apply, columns=columns)(jnp.ones(self.value.input_shape))\n",
    "    r=  f\"{s}\\n\"\n",
    "    s = '\\n'.join(self.__repr__().split('\\n')[1:])\n",
    "    r+= f\"{s}\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| Input        | Module                  | Module params   | Output      |   Param count |\n",
      "+==============+=========================+=================+=============+===============+\n",
      "| f32[500,784] | mlp (MLP)               |                 | f32[500,10] |        39,760 |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| f32[500,784] | mlp/~/linear_0 (Linear) | w: f32[784,50]  | f32[500,50] |        39,250 |\n",
      "|              |  └ mlp (MLP)            | b: f32[50]      |             |               |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "| f32[500,50]  | mlp/~/linear_1 (Linear) | w: f32[50,10]   | f32[500,10] |           510 |\n",
      "|              |  └ mlp (MLP)            | b: f32[10]      |             |               |\n",
      "+--------------+-------------------------+-----------------+-------------+---------------+\n",
      "+---------------------------------------------+---------+-------------+\n",
      "| Params                                      | State   | Callbacks   |\n",
      "+=============================================+=========+=============+\n",
      "| mlp/~/linear_0:                             | {}      | []          |\n",
      "|   b: all_zeros                              |         |             |\n",
      "|   w: x∈[-0.071, 0.071] μ=-9.844e-05 σ=0.032 |         |             |\n",
      "| mlp/~/linear_1:                             |         |             |\n",
      "|   b: all_zeros                              |         |             |\n",
      "|   w: x∈[-0.275, 0.279] μ=-0.002 σ=0.123     |         |             |\n",
      "+---------------------------------------------+---------+-------------+\n"
     ]
    }
   ],
   "source": [
    "print(ms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `callback` is any `Callable` that you pass on `subscribe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: callback 1\n"
     ]
    }
   ],
   "source": [
    "u1 = ms.subscribe(lambda x: print(\"1: callback 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the store value, triggers all callbacks subscribed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: callback 1\n"
     ]
    }
   ],
   "source": [
    "m = ms.get()\n",
    "ms.set(Model(**(m._asdict()|{\"state\": {'a': 1, 'b': 2}})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have different `stores` for different things.  For example, this is a simpler one to deal with the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Optimizer(NamedTuple):\n",
    "    state: optax.OptState\n",
    "    apply: Callable\n",
    "\n",
    "OptimizerStore = Writable[Optimizer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we will use [Optax](https://optax.readthedocs.io/), which is a good companion for `Haiku`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optimizer(state=(EmptyState(), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f02b868d670>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_tfm = optax.sgd(1e-3)\n",
    "apply = grad_tfm.update\n",
    "optState = grad_tfm.init(m.params) # you initialize the optimizer with the model params\n",
    "optimizer = Optimizer(state=optState, apply=apply)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Optimizer(state=(EmptyState(), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f02b868d670>)\n"
     ]
    }
   ],
   "source": [
    "os= OptimizerStore(optimizer)\n",
    "u2 = os.subscribe(lambda x: print(f\"callback 2: {x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Optimizer(state=(ScaleByAdamState(count=Array i32 gpu:0 0, mu={'mlp/~/linear_0': {'b': Array[50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[784, 50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'mlp/~/linear_1': {'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}}, nu={'mlp/~/linear_0': {'b': Array[50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0, 'w': Array[784, 50] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}, 'mlp/~/linear_1': {'b': Array[10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0 [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], 'w': Array[50, 10] \u001b[38;2;127;127;127mall_zeros\u001b[0m gpu:0}}), EmptyState()), apply=<function chain.<locals>.update_fn at 0x7f02b868d9d0>)\n"
     ]
    }
   ],
   "source": [
    "grad_tf2 = optax.adam(1e-4)\n",
    "optState2 = grad_tf2.init(m.params)\n",
    "os.set(Optimizer(state=optState2, apply=grad_tf2.update))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up... you should remember to unsubscribe when you are done with a store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1(), u2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m = Model.from_haiku(transformed=network, x=batch.input)\n",
    "ms = ModelStore(m)\n",
    "u1 = ms.subscribe(lambda x: print(f\"cb 1:\\n{x}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we arrived at the Training, the  `core` of the `core`  ```¯\\_(ツ)_/¯```\n",
    "\n",
    "Here is where we will most need callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in `fastai`, we create a `Learner` class that will deal with the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "LossFn = Callable[[Tensor, Tensor], Tensor] # per example loss function\n",
    "class Learner:\n",
    "    '''Basic class for handling the training loop.'''\n",
    "    def __init__(self, model:ModelStore, dls: DataLoaders, loss_func: LossFn, optimizer: OptimizerStore) -> None:\n",
    "        # keeping fastai orderhere. I would prefer: dls, model, optimizer, loss_func, which seems more natural to me.\n",
    "        fc.store_attr()\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}:\\n{self}\"\n",
    "    def __str__(self) -> str:\n",
    "        table = [[\"Model\", \"DataLoaders\", \"LossFn\", \"Optimizer\"],[id(self.model), id(self.dls), id(self.loss_func), id(self.optimizer)]]\n",
    "        return tabulate(table, headers='firstrow', tablefmt='grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner:\n",
       "+-----------------+-----------------+-----------------+-----------------+\n",
       "|           Model |     DataLoaders |          LossFn |       Optimizer |\n",
       "+=================+=================+=================+=================+\n",
       "| 139649660759008 | 139654223483664 | 139654224722384 | 139649661204080 |\n",
       "+-----------------+-----------------+-----------------+-----------------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = Learner(model=ms, dls=dls, loss_func=optax.softmax_cross_entropy_with_integer_labels, optimizer=os)\n",
    "learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learner itself, is not a store, but holds different stores for different aspects of the training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a `ModelStore`, an `OptimizerStore`... it is only missing the most important thing we want to __observe__... the training loop itself. We need a `TrainingStore`.\n",
    "\n",
    "But for that... let's first examine what we need.  Let's take a look in the __training loop__:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Fit`: the training Loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# pseudo-code\n",
    "\n",
    "def fit(epochs: int)->None:\n",
    "    '''Train the model for a number of epochs.'''\n",
    "    # before fit\n",
    "    for epoch in range(epochs):\n",
    "        # is_training\n",
    "        one_epoch(dls.train) # train for one epoch\n",
    "        # is_validating\n",
    "        one_epoch(dls.valid) # validate for one epoch\n",
    "        # should halt epochs?\n",
    "    # after fit\n",
    "\n",
    "def one_epoch(dl)->None:\n",
    "    '''Train or validate for one epoch.'''\n",
    "    # before epoch\n",
    "    for batch_n, batch in enumerate(dl): \n",
    "        one_batch(batch_n, batch)\n",
    "        # should halt batches?\n",
    "    # after epoch\n",
    "\n",
    "def one_batch(batch_n: int, batch: Batch)->None:\n",
    "    '''Train or validate for one batch.'''\n",
    "    # before batch\n",
    "    predict(...) # preds\n",
    "    evaluate(...)# loss\n",
    "    update model(...) if is_training\n",
    "    # after batch\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `TrainingStore` shall tell us where we are in the training loop and some information relevant at this point.\n",
    "\n",
    ">  I am `training`, `epoch` 5, `iteration` 345, after `evaluate` with certain `current loss`.\n",
    "\n",
    "\n",
    "Another aspect is that it seems it should be a `Readable` store, afterall, we don't want any callback being able to change information like:\n",
    "`in which batch of which epoch am I?`\n",
    "\n",
    "Exceptionally, we want to tell the `TrainingStore` to halt.\n",
    "\n",
    "Let's start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainingState(NamedTuple):\n",
    "\n",
    "    epochs: int                     # number of epochs to fit\n",
    "    epoch: int                      # current epoch\n",
    "    step: int                       # current step, since the beginning of the training\n",
    "    iter: int                       # current batch number, since beggining of the epoch\n",
    "    batch: Optional[Batch]          # current batch instance or None (if training hasn't started yet)\n",
    "\n",
    "    last: Dict=None           # last event that happened {'event': {payload}),\n",
    "                                    # eg. {'before_batch': {'iter': 345}}\n",
    "    is_running: bool=False          # True if running (training/validation), False if stopped\n",
    "    is_training: bool=False         # True if training is in progress\n",
    "    is_validating: bool=False       # True if evaluation is in progress\n",
    "    should_halt: bool=False         # True if should stop, False otherwise\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}:\\n{self}\"\n",
    "    def __str__(self) -> str:\n",
    "        return tabulate(list(self._asdict().items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingState:\n",
       "-------------  -----\n",
       "epochs             0\n",
       "epoch              0\n",
       "step               0\n",
       "iter               0\n",
       "batch\n",
       "last\n",
       "is_running     False\n",
       "is_training    False\n",
       "is_validating  False\n",
       "should_halt    False\n",
       "-------------  -----"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TrainingState(epochs=0, epoch=0, step=0, iter=0, batch=None)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from reax.stores import Readable\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStore(Readable[TrainingState]):\n",
    "    ''' A store that keeps tracking of the training loop state'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrainingStore representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+----+------+\n",
      "|    |   H1 |    |   H2 |\n",
      "+====+======+====+======+\n",
      "| A  |    1 | D  |    4 |\n",
      "+----+------+----+------+\n",
      "| B  |    2 | E  |    5 |\n",
      "+----+------+----+------+\n",
      "| C  |    3 |    |      |\n",
      "+----+------+----+------+\n"
     ]
    }
   ],
   "source": [
    "# a = [(\"A\", \"B\", \"C\"), (1,2,3)]\n",
    "# b = [(\"D\", \"E\", None), (4,5,None)]\n",
    "a = [(\"A\", 1), [\"B\", 2], [\"C\", 3]]\n",
    "b = [[\"D\", 4], [\"E\", 5]]\n",
    "c = list(zip(*a))\n",
    "d = list(zip(*b))\n",
    "table = list(itertools.zip_longest(*c,*d))\n",
    "print(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|    |   H1 |\n",
      "+====+======+\n",
      "| A  |    1 |\n",
      "+----+------+\n",
      "| B  |    2 |\n",
      "+----+------+\n",
      "| C  |    3 |\n",
      "+----+------+\n"
     ]
    }
   ],
   "source": [
    "a = [[\"A\", 1], [\"B\", 2], [\"C\", 3]]\n",
    "b = []\n",
    "c = list(zip(*a))\n",
    "d = list(zip(*b))\n",
    "table = list(itertools.zip_longest(*c,*d))\n",
    "print(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----+---------------------------------------+\n",
      "|             |    H1 |    | H2                                    |\n",
      "+=============+=======+====+=======================================+\n",
      "| epoch       |     0 | 0: | <function <lambda> at 0x7f02b80065e0> |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| step        |     0 |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| batch_n     |     0 |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| batch       |       |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| metrics     |       |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| last_event  |       |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| is_training | False |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n",
      "| should_halt | False |    |                                       |\n",
      "+-------------+-------+----+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "a = [('epoch', 0), ('step', 0), ('batch_n', 0), ('batch', None), ('metrics', None), ('last_event', None), ('is_training', False), ('should_halt', False)]\n",
    "b = [('0:', lambda:None)]\n",
    "c = list(zip(*a))\n",
    "d = list(zip(*b))\n",
    "table = list(itertools.zip_longest(*c,*d))\n",
    "print(tabulate(table, headers=['','H1','', \"H2\"], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def __repr__(self: TrainingStore) -> str:\n",
    "        return f\"{self.__class__.__name__}:\\n{self}\"\n",
    "@fc.patch\n",
    "def __str__(self: TrainingStore) -> str:\n",
    "    state = list(self.value._asdict().items())\n",
    "    state_t = list(zip(*state))\n",
    "    cbs = [(f\"{i}:\", v) for i, v in enumerate(self.subscribers)]\n",
    "    cbs_t = list(zip(*cbs))\n",
    "    table = list(itertools.zip_longest(*state_t,*cbs_t))\n",
    "    return tabulate(table, headers=['','State','', 'Calbacks'], tablefmt='grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback 4:\n",
      " -------------  -----\n",
      "epochs             0\n",
      "epoch              0\n",
      "step               0\n",
      "iter               0\n",
      "batch\n",
      "last\n",
      "is_running     False\n",
      "is_training    False\n",
      "is_validating  False\n",
      "should_halt    False\n",
      "-------------  -----\n"
     ]
    }
   ],
   "source": [
    "ts = TrainingStore(t, lambda x:None)\n",
    "u4 = ts.subscribe(lambda x: print(f\"callback 4:\\n {x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+----+---------------------------------------+\n",
      "|               |   State |    | Calbacks                              |\n",
      "+===============+=========+====+=======================================+\n",
      "| epochs        |       0 | 0: | <function <lambda> at 0x7f02b802a790> |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| epoch         |       0 |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| step          |       0 |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| iter          |       0 |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| batch         |         |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| last          |         |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| is_running    |   False |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| is_training   |   False |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| is_validating |   False |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n",
      "| should_halt   |   False |    |                                       |\n",
      "+---------------+---------+----+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingStore(Writable[TrainingState]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def fit(self:Learner, n_epochs, trnState: TrainingState):\n",
    "    \"Fit the model for `n_epochs` using batches from `dls`\"\n",
    "    trnState.emit(Event(id=\"before_fit\", payload=None))\n",
    "    for epoch in range(n_epochs):\n",
    "        self.one_epoch(is_training=True, trnState=trnState)\n",
    "        self.one_epoch(is_training=False, trnState=trnState)\n",
    "        if (trnState.get().should_halt): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = TrainingStore(TrainingState(epoch=0, step=0, batch_n=0, batch=None, metrics=None, last_event=None))\n",
    "u3 = training.subscribe(lambda x: print(f\"3:\\n {x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    training._.epoch = 1\n",
    "except:\n",
    "    print('should fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training._ = TrainingState(epoch=1, step=0, batch_n=0, batch=None, metrics=None, last_event=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubs = []\n",
    "for i in range(12):\n",
    "    u = training.subscribe(lambda x: print(f\"callback: {i}\"))\n",
    "    unsubs.append(u)\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in unsubs:u()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def fit(self:Learner, n_epochs, trnState: TrainingState):\n",
    "    \"Fit the model for `n_epochs` using batches from `dls`\"\n",
    "    trnState.emit(Event(id=\"before_fit\", payload=None))\n",
    "    for epoch in range(n_epochs):\n",
    "        self.one_epoch(is_training=True, trnState=trnState)\n",
    "        self.one_epoch(is_training=False, trnState=trnState)\n",
    "        if (trnState.get().should_halt): break\n",
    "\n",
    "@fc.patch\n",
    "def one_epoch(self:Learner, is_training: bool, trnState: TrainingState):\n",
    "    a = 1\n",
    "    # print(f\"one_epoch: is_training={is_training}\")\n",
    "    # print(trnState)\n",
    "    # trnState._s_is_training = is_training\n",
    "    # self.dl = self.dls.train if is_training else self.dls.valid\n",
    "    # trnState.emit(Event(id=f\"before_epoch\", payload=trnState._s_epoch))\n",
    "    # for batch_n, batch in enumerate(self.dl):\n",
    "    #     trnState._s_batch_n, trnState._s_batch  = batch_n, batch\n",
    "    #     # self.one_batch(trnState=trnState)\n",
    "    #     if (trnState._s_should_halt): break\n",
    "    # trnState.emit(Event(id=f\"after_epoch\", payload=trnState._s_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params, state, apply, _ = ms.get()\n",
    "rng = hk.PRNGSequence(42) # random number generator\n",
    "@jax.jit\n",
    "def _predict(params, state, key, batch) -> Tensor:\n",
    "    logits, new_state = apply(params, state, key, batch.input)\n",
    "    return jnp.argmax (logits, axis=-1), new_state\n",
    "key = next(rng)\n",
    "_predict(params, state, key, batch)\n",
    "@jax.jit\n",
    "def _evaluate(params, state, key, batch) -> Tensor:\n",
    "    preds, _ = _predict(params, state, key, batch)\n",
    "    return jnp.mean(preds == batch.target)\n",
    "from torch.utils.benchmark import Timer\n",
    "evTimer = Timer(stmt=\"_evaluate(params, state, key, batch)\", globals=globals())\n",
    "evTimer.timeit(1000)\n",
    "rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
    "\n",
    "def evaluate(model: ModelStore, batch: Batch) -> Tensor:\n",
    "    params, state, apply, _ = model.get()\n",
    "    key = next(rng)\n",
    "    return _evaluate(params, state, key, batch)\n",
    "evaluate(ms, batch)\n",
    "@jax.jit\n",
    "def _loss_fn(params, state, key, batch)-> jnp.ndarray:\n",
    "    targs = batch.target\n",
    "    preds, new_state = apply(params, state, key, batch.input)\n",
    "    # return the expectation of the loss wrt the distribution of the targets\n",
    "    return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(preds, targs)/targs.shape[0]), new_state\n",
    "key = next(rng)\n",
    "loss, new_state = _loss_fn(params, state, key, batch)\n",
    "lfTimer = Timer(stmt=\"_loss_fn(params, state, key, batch)\", globals=globals())\n",
    "lfTimer.timeit(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnState = TrainingStore(TrainingState(epoch=0, step=0, batch_n=0, batch=None, metrics=None, last_event=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnState.get().should_halt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u4 = trnState.subscribe(lambda x: print(f\"4:\\n {x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1, trnState=trnState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = NamedTuple('A', [('a', int), ('b', int)])(1,2)\n",
    "# b = NamedTuple('A', [('a', int), ('b', int)])(3,3)\n",
    "# s1 = set(a._asdict().items())\n",
    "# s2 = set(b._asdict().items())\n",
    "# s1 ^ s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trnState = TrainingStore(TrainingState(epoch=0, step=0, batch_n=0, batch=None, metrics=None, last_event=None))\n",
    "# logs = []\n",
    "# def logger(x):\n",
    "#     logs.append(x)\n",
    "#     last = set((logs[-1])._asdict().items())\n",
    "#     curr = set((x)._asdict().items())\n",
    "#     print (last ^ curr)\n",
    "\n",
    "# u4 = trnState.subscribe(lambda x: logger(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u4 = trnState.subscribe(lambda x: print(f\"4: {x}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnState._s_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2, trnState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch(self):\n",
    "    self.preds = self.model(self.batch[0])\n",
    "    self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "    if self.model.training:\n",
    "        self.loss.backward()\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @with_cbs('batch')\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback('after_predict')\n",
    "        self.get_loss()\n",
    "        self.callback('after_loss')\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback('after_backward')\n",
    "            self.step()\n",
    "            self.callback('after_step')\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.dls.train if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: self.one_epoch(True)\n",
    "            if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None: lr = self.lr\n",
    "            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
    "params, state, apply, _ = ms.get()\n",
    "@jax.jit\n",
    "def _loss_fn(params, state, batch)-> Tuple[jnp.ndarray, PyTree]:\n",
    "    bs, *_ = batch.target.shape\n",
    "    logits, state = apply(params, state, next(rng), batch.input)\n",
    "    state = {'a':1, 'b':2}\n",
    "    return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(logits, batch.target)/bs)\n",
    "\n",
    "def loss_fn(model: ModelStore, batch: Batch) -> float:\n",
    "    params, state, apply, _ = model.get()\n",
    "    loss_value =  _loss_fn(params, state, batch)\n",
    "    new_model = Model(**(m._asdict()|{'state': new_state}))\n",
    "    model.set(new_model)\n",
    "    return float(loss_value)\n",
    "\n",
    "loss_fn(ms, batch)\n",
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
    "\n",
    "def update(model: ModelStore, optimizer: OptimizerStore, batch: Batch)->None:\n",
    "    m = model.get()\n",
    "    o = optimizer.get()\n",
    "    f = partial(loss_fn)(model=model)\n",
    "    grads = jax.grad(loss_fn)(batch)\n",
    "    @jax.jit\n",
    "    def _update():\n",
    "        updates, new_optState = o.apply(grads, o.state)\n",
    "        new_model_params = optax.apply_updates(m.params, updates)\n",
    "        return new_model_params, new_optState\n",
    "    new_model_params, new_optState = _update()\n",
    "    new_model = Model(**(m._asdict()|{'params': new_model_params}))\n",
    "    new_optimizer = Optimizer(**(o._asdict()|{'state': new_optState}))\n",
    "    model.set(new_model)\n",
    "    optimizer.set(new_optimizer)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: tentar jax.tree_util.Partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ms.get()\n",
    "o = os.get()\n",
    "f = partial(loss_fn, model=ms)\n",
    "grads = jax.grad(f)(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
    "params, state, apply, _ = ms.get()\n",
    "def loss_fn():\n",
    "    loss_value, new_state =  _loss_fn(params, state, batch)\n",
    "    \n",
    "grads = jax.grad(_loss_fn)(params, state, batch)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(ms, os, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = hk.PRNGSequence(jax.random.PRNGKey(42))\n",
    "\n",
    "# def loss_fn(model: ModelStore, batch: Batch) -> float:\n",
    "#     params, state, apply, _ = model.get()\n",
    "#     @jax.jit\n",
    "#     def _loss(params, state, batch)-> jnp.ndarray:\n",
    "#         bs, *_ = batch.target.shape\n",
    "#         logits, state = (apply)(params, state, next(rng), batch.input)\n",
    "#         return jnp.sum(optax.softmax_cross_entropy_with_integer_labels(logits, batch.target)/bs), state\n",
    "#     loss_value, new_state =  _loss(params, state, batch)\n",
    "#     new_model = Model(**(m._asdict()|{'state': new_state}))\n",
    "#     model.set(new_model)\n",
    "#     return float(loss_value)\n",
    "\n",
    "# loss_fn(ms, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss(loss_func, *args): return jax.jit(lambda params: loss_func(get_model(params), *args))\n",
    "# mse_loss = get_loss(mse, xb,tb) \n",
    "# mse_loss, mse_loss(W)\n",
    "# from torch.utils.benchmark import Timer\n",
    "# jax_grad = Timer( stmt=\"jax.grad(mse_loss)\", globals=globals())\n",
    "# jax_grad.timeit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrainingStore(Writable[TrainingState]):\n",
    "\n",
    "#     def emit(self, event: Event):\n",
    "#         self.set(self.value._replace(last_event=event))\n",
    "#     # def __getattr__(self, name): # there  is a bug, I can't fi\n",
    "#     #     if name[:3]=='_s_' : return getattr(self.value, name[3:])\n",
    "#     #     else: return super().__getattr__(name)\n",
    "#     # def __setattr__(self, name, value):\n",
    "#     #     if name[:3]=='_s_' and hasattr(self.value, name[3:]):\n",
    "#     #         self.set(self.value._replace(**{name[3:]: value}))\n",
    "#     #     else: super().__setattr__(name, value)\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}:\\n{self}\"\n",
    "#     def __str__(self) -> str:\n",
    "#         state = list(self.value._asdict().items())\n",
    "#         cbs = [(f\"{i}:\", v) for i, v in enumerate(self.subscribers)]\n",
    "#         table = list(itertools.zip_longest(list(zip(*state)),list(zip(*cbs))))\n",
    "#         return tabulate(table, headers=['State', 'Calbacks'], tablefmt='grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def __repr__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}:\\n{self}\"\n",
    "#     def __str__(self) -> str:\n",
    "#         state = list(self.value._asdict().items())\n",
    "#         state_t = list(zip(*state))\n",
    "#         cbs = [(f\"{i}:\", v) for i, v in enumerate(self.subscribers)]\n",
    "#         cbs_t = list(zip(*cbs))\n",
    "#         table = list(itertools.zip_longest(*state_t,*cbs_t))\n",
    "#         return tabulate(table, headers=['','State','', 'Calbacks'], tablefmt='grid')\n",
    "#     # @property\n",
    "#     # def _(self):\n",
    "#     #     \"\"\"The store value.\"\"\"\n",
    "#     #     return self.value\n",
    "#     # @_.setter\n",
    "#     # def _(self, value: TrainingState):\n",
    "#     #     self.set(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxai3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "51b0b17cb22a1735ec1ccb0abdcaafa60a51a245238160acc2be70f4722b7533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
