# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['PyTree', 'ApplyFn', 'ModelStore', 'LossFn', 'learner', 'Observable', 'Optimizer', 'Learner', 'Training',
           'TrainingStore']

# %% ../nbs/00_core.ipynb 4
from functools import partial
from typing import (Callable, Dict, Hashable, List, Mapping, NamedTuple,
                    Optional, Sequence, Tuple, Union)

import haiku as hk
import jax
import jax.numpy as jnp
import lovely_jax as lj
import numpy as np
import optax
import torch
from torch.utils.data import DataLoader

# %% ../nbs/00_core.ipynb 14
import fastcore.all as fc
from tabulate import tabulate
from .utils import str_tree
# the tabulate package is also used by Haiku in its hk.experimental methods

# %% ../nbs/00_core.ipynb 16
PyTree = Union[
    Tensor, Tuple["PyTree", ...], List["PyTree"], Dict[Hashable, "PyTree"], hk.Params, hk.State, optax.OptState, None
]  # I hope that with this definition it will work in  Haiku and Flax

ApplyFn = Callable[..., Tuple[Tensor, PyTree]] # returns result and state (aka buffers)

# %% ../nbs/00_core.ipynb 18
class Observable(StoreProtocol):
    def __init__(self, /, **kw):
        self.__store = writable(kw)
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}<{len(self.__store)}>:{self.__store.get()!r}"
    def __str__(self) -> str:
        return str(self.__store.get())
    def __getattr__(self, k:str):
        return self.__store.get()[k]
    def __setattr__(self, k:str,v) -> None:
        if f'_{self.__class__.__name__}_' in k: super().__setattr__(k,v)
        else: self.__store.set(self.__store.get() | {k:v})
    def subscribe(self, callback:Subscriber) -> Unsubscriber:
        return self.__store.subscribe(callback)

# %% ../nbs/00_core.ipynb 37
class Optimizer(NamedTuple):
    params: PyTree # optimizer hyperparams (e.g. learning rate)
    state: optax.OptState # the optimizer state
    run: Callable # the optimizer update function

    rng = hk.PRNGSequence(42)
    @staticmethod
    def from_optax(
        o: optax.GradientTransformation,       # optax object
        p: PyTree,                             # model parameters
    ):
        ''' Create an Optimizer from an Optax Gradient Transformation object.'''
        state = o.init(p)
        return Optimizer(params=params, state=state, run=apply, input_shape=x.shape)


# %% ../nbs/00_core.ipynb 43
from sveltish.stores import Writable, Notifier, Readable, writable, readable, Store, derived, Subscriber, Updater

# %% ../nbs/00_core.ipynb 45
ModelStore = Writable[Model] # a store that holds a Model
# class ModelStore(Store[Model]):
#     ''' A Model store. Custom Writable store'''
#     def __init__(self,
#                 initial_value: Model, # Initial value of the store
#             ) -> None:
#         start: Notifier = lambda x: None # we won't need a Start/Stop Notifier
#         super().__init__(initial_value, start)

# %% ../nbs/00_core.ipynb 48
import yaml


# %% ../nbs/00_core.ipynb 49
@fc.patch
def __repr__(self:ModelStore) -> str:
    params, state, apply, shape = self.value
    table = [["Params", "State", "Callbacks"],[str_tree(params), str_tree(state), [f"{s}\n" for s in self.subscribers]]]
    table = [["Params", "State", "Callbacks"],
                [str_tree(m.params), str_tree(m.state), yaml.dump([{i:str(f)} for i,f in enumerate(self.subscribers)])]]
    return f"{self.__class__.__name__}:\n{tabulate(table, headers='firstrow', tablefmt='grid')}"

# %% ../nbs/00_core.ipynb 51
@fc.patch
def __str__(self:ModelStore) -> str:
    columns=["input", "module", "owned_params", "output", "params_size"]
    s = hk.experimental.tabulate(self.value.apply, columns=columns)(jnp.ones(self.value.input_shape))
    r=  f"{s}\n"
    s = '\n'.join(self.__repr__().split('\n')[1:])
    r+= f"{s}"
    return r

# %% ../nbs/00_core.ipynb 65
LossFn = Callable[[Tensor, Tensor], Tensor] # per example loss function
class Learner:
    '''Basic class for handling the training loop.'''
    def __init__(self, model:ModelStore, dls: DataLoaders, loss_func: LossFn, optimizer: OptimizerStore) -> None:
        # keeping fastai order here. I would prefer: dls, model, optimizer, loss_func, which seems more natural to me.
        fc.store_attr()
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}:\n{self}"
    def __str__(self) -> str:
        table = [["Model", "DataLoaders", "LossFn", "Optimizer"],[id(self.model), id(self.dls), id(self.loss_func), id(self.optimizer)]]
        return tabulate(table, headers='firstrow', tablefmt='grid')

learner = Learner(model=ms, dls=dls, loss_func=optax.softmax_cross_entropy_with_integer_labels, optimizer=os)
learner

# %% ../nbs/00_core.ipynb 71
class Training(NamedTuple):

    last: Tuple                     # last event that happened ('event': (payload)),
                                    # eg. {'before_batch': {'iter': 345}}
    epochs: int                     # number of epochs to fit
    epoch: int                      # current epoch
    step: int                       # current step, since the beginning of the training
    iter: int                       # current batch number, since beggining of the epoch
    batch: Optional[Batch]          # current batch instance or None (if training hasn't started yet)
    is_running: bool=False          # True if running (training/validation), False if stopped
    is_training: bool=False         # True if training is in progress
    is_validating: bool=False       # True if evaluation is in progress
    should_halt: bool=False         # True if should stop, False otherwise

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}:\n{self}"
    def __str__(self) -> str:
        s = ""
        for (start,end) in [(0,6),(6,10)]:
            keys = list(t._asdict().keys())[start:end]
            values = [t._asdict()[key] for key in keys]
            s+= f"{tabulate([values], headers=keys, tablefmt='grid')}\n"
        return s

# %% ../nbs/00_core.ipynb 75
class TrainingStore(Store[Training]):
    ''' A store that keeps tracking of the training loop state'''
    def on(self, event: str, # event to subscribe to (eg. 'before_batch')
           callback: Subscriber, # callback to call when the event happens, receives event payload as argument
           ) -> Tuple[str, ...]: # return an event tuple, eg. ('before_batch', 4)
        '''Subscribe to a specific event'''
        def _callback(x):
            callback(getattr(self.value, 'last'))
        # subscribe callback to derived store that will be updated when the event happens
        return (self | (lambda x: getattr(x, 'last')) | (lambda x: (x[0] == event, x[1])) ).subscribe(_callback)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__} <{len(self.subscribers)}>:\n{self.value}"
