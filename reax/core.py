# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['PyTree', 'ApplyFn', 'OptimizerStore', 'LossFn', 'Model', 'ModelStore', 'Optimizer', 'Learner', 'TrainingState',
           'TrainingStore']

# %% ../nbs/00_core.ipynb 4
from functools import partial
from typing import (Callable, Dict, Hashable, List, Mapping, NamedTuple,
                    Optional, Sequence, Tuple, Union)

import haiku as hk
import jax
import jax.numpy as jnp
import lovely_jax as lj
import numpy as np
import optax
import torch
from torch.utils.data import DataLoader

# %% ../nbs/00_core.ipynb 14
PyTree = Union[
    Tensor, Tuple["PyTree", ...], List["PyTree"], Dict[Hashable, "PyTree"], hk.Params, hk.State, optax.OptState, None
]  # I hope that with this definition it will work in  Haiku and Flax

ApplyFn = Callable[..., Tuple[Tensor, PyTree]] # returns result and state (aka buffers)

class Model(NamedTuple):
    params: PyTree # the models parameters, weights and biases
    state: PyTree  # the model auxiliary state, e.g. batchnorm buffers
    apply: ApplyFn # the model forward pass function
    input_shape: Tuple[int, ...] # the shape of the input, used to infer the model output shape

    rng = hk.PRNGSequence(42) # random number generator

    @staticmethod
    def from_haiku(
        transformed: hk.TransformedWithState,       # transformed haiku model
        x: Tensor                                   # example input (e.g. batch.input)
    ):
        ''' Create a Model from a Haiku Transformed object and an example input.'''
        init, apply = transformed
        params, state = jax.jit(init)(next(Model.rng), x)
        return Model(params=params, state=state, apply=apply, input_shape=x.shape)

# %% ../nbs/00_core.ipynb 17
import fastcore.all as fc
from tabulate import tabulate
from .utils import str_tree
# the tabulate package is also used by Haiku in its hk.experimental methods

# %% ../nbs/00_core.ipynb 18
@fc.patch
def __repr__(self:Model)->str:
    table = [["Params", "State"],[str_tree(self.params), str_tree(self.state)]]
    return f"{self.__class__.__name__}:\n{tabulate(table, headers='firstrow', tablefmt='grid')}"


# %% ../nbs/00_core.ipynb 20
@fc.patch
def __str__(self:Model) -> str:
    s1 = hk.experimental.tabulate(self.apply,
            columns=["input", "module", "owned_params", "output", "params_size"])(jnp.ones(self.input_shape))
    s2 = '\n'.join(self.__repr__().split('\n')[1:])
    return f"{s1}\n{s2}"

# %% ../nbs/00_core.ipynb 24
from .stores import Writable, Notifier

# %% ../nbs/00_core.ipynb 26
class ModelStore(Writable[Model]):
    ''' A Model store. Custom Writable store'''
    def __init__(self,
                initial_value: Model, # Initial value of the store
            ) -> None:
        start: Notifier = lambda x: None # we won't need a Start/Stop Notifier
        super().__init__(initial_value, start)

# %% ../nbs/00_core.ipynb 29
import yaml


# %% ../nbs/00_core.ipynb 30
@fc.patch
def __repr__(self:ModelStore) -> str:
    params, state, apply, shape = self.value
    table = [["Params", "State", "Callbacks"],[str_tree(params), str_tree(state), [f"{s}\n" for s in self.subscribers]]]
    table = [["Params", "State", "Callbacks"],
                [str_tree(m.params), str_tree(m.state), yaml.dump([{i:str(f)} for i,f in enumerate(self.subscribers)])]]
    return f"{self.__class__.__name__}:\n{tabulate(table, headers='firstrow', tablefmt='grid')}"

# %% ../nbs/00_core.ipynb 32
@fc.patch
def __str__(self:ModelStore) -> str:
    columns=["input", "module", "owned_params", "output", "params_size"]
    s = hk.experimental.tabulate(self.value.apply, columns=columns)(jnp.ones(self.value.input_shape))
    r=  f"{s}\n"
    s = '\n'.join(self.__repr__().split('\n')[1:])
    r+= f"{s}"
    return r

# %% ../nbs/00_core.ipynb 40
class Optimizer(NamedTuple):
    state: optax.OptState
    apply: Callable

OptimizerStore = Writable[Optimizer]

# %% ../nbs/00_core.ipynb 52
LossFn = Callable[[Tensor, Tensor], Tensor] # per example loss function
class Learner:
    '''Basic class for handling the training loop.'''
    def __init__(self, model:ModelStore, dls: DataLoaders, loss_func: LossFn, optimizer: OptimizerStore) -> None:
        # keeping fastai orderhere. I would prefer: dls, model, optimizer, loss_func, which seems more natural to me.
        fc.store_attr()
    def __repr__(self) -> str:
        return f"{self.__class__.__name__}:\n{self}"
    def __str__(self) -> str:
        table = [["Model", "DataLoaders", "LossFn", "Optimizer"],[id(self.model), id(self.dls), id(self.loss_func), id(self.optimizer)]]
        return tabulate(table, headers='firstrow', tablefmt='grid')

# %% ../nbs/00_core.ipynb 59
class TrainingState(NamedTuple):

    epochs: int                     # number of epochs to fit
    epoch: int                      # current epoch
    step: int                       # current step, since the beginning of the training
    iter: int                       # current batch number, since beggining of the epoch
    batch: Optional[Batch]          # current batch instance or None (if training hasn't started yet)

    last: Dict=None           # last event that happened {'event': {payload}),
                                    # eg. {'before_batch': {'iter': 345}}
    is_running: bool=False          # True if running (training/validation), False if stopped
    is_training: bool=False         # True if training is in progress
    is_validating: bool=False       # True if evaluation is in progress
    should_halt: bool=False         # True if should stop, False otherwise

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}:\n{self}"
    def __str__(self) -> str:
        return tabulate(list(self._asdict().items()))

# %% ../nbs/00_core.ipynb 61
from .stores import Readable
import itertools

# %% ../nbs/00_core.ipynb 62
class TrainingStore(Readable[TrainingState]):
    ''' A store that keeps tracking of the training loop state'''

# %% ../nbs/00_core.ipynb 67
@fc.patch
def __repr__(self: TrainingStore) -> str:
        return f"{self.__class__.__name__}:\n{self}"
@fc.patch
def __str__(self: TrainingStore) -> str:
    state = list(self.value._asdict().items())
    state_t = list(zip(*state))
    cbs = [(f"{i}:", v) for i, v in enumerate(self.subscribers)]
    cbs_t = list(zip(*cbs))
    table = list(itertools.zip_longest(*state_t,*cbs_t))
    return tabulate(table, headers=['','State','', 'Calbacks'], tablefmt='grid')
